import pandas as pd
from datetime import datetime
import numpy as np
import matplotlib.pyplot as plt
import os
from sklearn.model_selection import train_test_split
import lightgbm as lgb
import xgboost as xgb
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score


df_train = pd.read_csv('/dataset/signate_beginner34/train.csv')
df_test = pd.read_csv('/dataset/signate_beginner34/test.csv')
sample_submission = pd.read_csv('/dataset/signate_beginner34/sample_submission.csv')


df_train.columns


def preprocessing(df):
    data = df.copy()
    data['sc_size'] = data['sc_h'] * data['sc_w']
    data['spec'] = data['clock_speed'] * data['n_cores']

    feature = ['int_memory', 'm_dep', 'mobile_wt',
               'pc', 'px_height', 'px_width', 'ram',
               'spec']
        # feature = ['battery_power', 'clock_speed', 'dual_sim', 'fc',
        #        'int_memory', 'm_dep', 'mobile_wt', 'n_cores',
        #        'pc', 'px_height', 'px_width', 'ram', 'sc_h', 'sc_size']
    return data[feature]


def f1(y_pred, train_data):
    N_LABELS = 4
    reshaped_preds = y_pred.reshape(N_LABELS, len(y_pred) // N_LABELS)
    y_pred_ = reshaped_preds.argmax(axis=0)
    y_true = train_data.get_label()
    score = f1_score(y_true, y_pred_, average='macro')
    return 'f1', score, True

def get_feature_importance(target_columms=None):
    '''
    特徴量の出力
    '''
    feature_imp = pd.DataFrame(sorted(zip(self.model.feature_importance(), self.target_columms)), columns=['Value','Feature'])
    return feature_imp


X = preprocessing(df_train)
y = df_train['price_range']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test)

params = {
    'objective': 'multiclass', # 多クラス分類
    'metric': 'multi_logloss', # 損失関数にmulti_loglossを使用
    'num_class': 4, # クラスの数
    'boosting_type': 'gbdt',
    'lambda_l1': 0.2,
    'lambda_l2': 0.2,
    'learning_rate':0.01,
    'drop_rate':0.5,
    # 'max_depth': 3,
    'verbosity': -1,
}
verbose_eval = 0  # この数字を1にすると学習時のスコア推移がコマンドライン表示される
lgb_model = lgb.train(params,
                  train_data,
                  num_boost_round=10000,
                  valid_sets=[train_data, test_data],
                  feval=f1,
                  callbacks=[
                      lgb.early_stopping(stopping_rounds=10, verbose=True),
                      lgb.log_evaluation(verbose_eval) # この数字を1にすると学習時のスコア推移がコマンドライン表示される
                  ])

y_pred = lgb_model.predict(X_test)
y_pred_class = np.argmax(y_pred, axis=1)
y_pred_class


from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, y_pred_class))

score = f1_score(y_test, y_pred_class, average='macro')
score


plt.figure(figsize=(10, 6))
lgb.plot_importance(lgb_model)
plt.show()


train_data = xgb.DMatrix(X_train, label=y_train)
test_data = xgb.DMatrix(X_test, label=y_test)

params = {
    'objective': 'multi:softprob', # 多クラス分類
    'num_class': 4, # クラスの数
    'booster': 'gbtree',
    'learning_rate':0.01,
    'max_depth': 5,
}
history = {}
verbose_eval = 0  # この数字を1にすると学習時のスコア推移がコマンドライン表示される
xgb_model = xgb.train(params,
                  train_data,
                  num_boost_round=10000,
                  # feval=f1,
                  early_stopping_rounds=50,
                  evals=[(train_data, "train"), (test_data, "test")],
                  evals_result=history,
                 )

y_pred = xgb_model.predict(test_data)
y_pred_class = np.argmax(y_pred, axis=1)
y_pred_class


print(accuracy_score(y_test, y_pred_class))

score = f1_score(y_test, y_pred_class, average='macro')
score


xgb.plot_importance(xgb_model)


# from sklearn.model_selection import KFold
# # クロスバリデーション用のScikit-Learnクラス（5分割KFold）
# cv = KFold(n_splits=5, shuffle=True, random_state=42)

# ###### ここからがLightGBMの実装 ######
# # データをDatasetクラスに格納
# dcv = lgb.Dataset(X, label=y)  # クロスバリデーション用
# # 使用するパラメータ
# params = {
#     'objective': 'multiclass', # 多クラス分類
#     'num_class': 4, # クラスの数
#     'metric': 'multi_logloss', # 損失関数にmulti_loglossを使用
#     'random_state': 42,  # 乱数シード
#     'boosting_type': 'gbdt',
#     'reg_alpha': 0.0,
#     'reg_lambda': 0.0,
#     'learning_rate':0.01, 
#     'drop_rate':0.5,
#     'verbose': -1
# }
# verbose_eval = 0  # この数字を1にすると学習時のスコア推移がコマンドライン表示される
# # early_stoppingを指定してLightGBMをクロスバリデーション
# cv_result = lgb.cv(params, dcv,
#                 num_boost_round=10000,  # 最大学習サイクル数。early_stopping使用時は大きな値を入力
#                 folds=cv,
#                 feval=f1,
#                 callbacks=[lgb.early_stopping(stopping_rounds=100, 
#                                 verbose=True), # early_stopping用コールバック関数
#                            lgb.log_evaluation(verbose_eval)] # コマンドライン出力用コールバック関数
#                 )
# # print(cv_result)
# print(f'multi logloss mean={cv_result["multi_logloss-mean"][-1]}')

# test_data = preprocessing(df_test)
# pred = ddmodel.predict(test_data)
# pred_class = np.argmax(pred, axis=1)
# pred_class


test_data = preprocessing(df_test)
# pred = model.predict(test_data)
# pred_class = np.argmax(pred, axis=1)

emsemble_test = pd.DataFrame({
    'XGB': np.argmax(xgb_model.predict(xgb.DMatrix(test_data)), axis=1),
    'LGB': np.argmax(lgb_model.predict(test_data), axis=1)
    })
emsemble_test


mean = emsemble_test['LGB'] * 0.8 + emsemble_test['XGB'] * 0.2
pred_class = mean.round().astype(int)
pred_class


# sample_submission['2'] = proba
submission = pd.concat([df_test['id'], pd.DataFrame(pred_class)], axis=1)
submission = submission.rename(columns={'id': 1})
submission


# 保存
save_folder = "results"
if not os.path.exists(save_folder):
    os.makedirs(save_folder)

submission.to_csv("{}/submit_{}.csv".format(save_folder, datetime.now().strftime("%Y-%m-%d-%H%M%S")),index=False, header=None)



