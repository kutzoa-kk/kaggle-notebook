{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-30T04:27:45.336844Z",
     "iopub.status.busy": "2023-11-30T04:27:45.336524Z",
     "iopub.status.idle": "2023-11-30T04:27:46.110799Z",
     "shell.execute_reply": "2023-11-30T04:27:46.110178Z",
     "shell.execute_reply.started": "2023-11-30T04:27:45.336830Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from warnings import simplefilter\n",
    "import lightgbm as lgb\n",
    "import catboost as cbt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T04:27:47.606846Z",
     "iopub.status.busy": "2023-11-30T04:27:47.606550Z",
     "iopub.status.idle": "2023-11-30T04:27:47.610594Z",
     "shell.execute_reply": "2023-11-30T04:27:47.610054Z",
     "shell.execute_reply.started": "2023-11-30T04:27:47.606833Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \"\"\"\n",
    "    Configuration class for parameters and CV strategy for tuning and training\n",
    "    Please use caps lock capital letters while filling in parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data preparation\n",
    "    version_nb         = 3\n",
    "    is_gpu             = True\n",
    "    # device             = torch.device('cuda' if torch.cuda.is_available() and gpu_switch else 'cpu')\n",
    "    state              = 42\n",
    "    num_workers        = 4\n",
    "\n",
    "    # BEFORE SUBMIT, CHECK SETTINGS\n",
    "    is_test_mode       = False\n",
    "    test_mode_frac     = 10\n",
    "    is_offline         = True\n",
    "    testing_days       = 2\n",
    "\n",
    "    target             = 'target'    \n",
    "    path               = '/kaggle/input/optiver-trading-at-the-close'\n",
    "    train_path         = f'{path}/train.csv'\n",
    "    test_path          = f'{path}/example_test_files/test.csv'\n",
    "    model_path         = f'' if not is_offline else f'{path}/'\n",
    "    \n",
    "    TRAINING           = True\n",
    "    INFERENCE          = True\n",
    "    TUNING             = False\n",
    "    \n",
    "    methods            = ['LGBM',]\n",
    "    # methods            = ['CBT']\n",
    "\n",
    "    plt_path = f'fig/turning'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Data Loading and Preprocessing \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T04:27:48.484596Z",
     "iopub.status.busy": "2023-11-30T04:27:48.484187Z",
     "iopub.status.idle": "2023-11-30T04:27:53.423802Z",
     "shell.execute_reply": "2023-11-30T04:27:53.423142Z",
     "shell.execute_reply.started": "2023-11-30T04:27:48.484574Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5237892, 17)\n"
     ]
    }
   ],
   "source": [
    "# 📂 Read the dataset from a CSV file using Pandas\n",
    "df = pd.read_csv(CFG.train_path)\n",
    "if CFG.is_test_mode:\n",
    "    df = df[df['stock_id'] < 10]\n",
    "\n",
    "# 🧹 Remove rows with missing values in the \"target\" column\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "\n",
    "# 🔁 Reset the index of the DataFrame and apply the changes in place\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 📏 Get the shape of the DataFrame (number of rows and columns)\n",
    "df_shape = df.shape\n",
    "print(df_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T04:27:53.430556Z",
     "iopub.status.busy": "2023-11-30T04:27:53.430452Z",
     "iopub.status.idle": "2023-11-30T04:27:53.435990Z",
     "shell.execute_reply": "2023-11-30T04:27:53.435539Z",
     "shell.execute_reply.started": "2023-11-30T04:27:53.430547Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "               \n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    " # Parallel Triplet Imbalance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T04:27:53.437060Z",
     "iopub.status.busy": "2023-11-30T04:27:53.436900Z",
     "iopub.status.idle": "2023-11-30T04:27:53.514117Z",
     "shell.execute_reply": "2023-11-30T04:27:53.513503Z",
     "shell.execute_reply.started": "2023-11-30T04:27:53.437050Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighbors Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T04:27:53.514824Z",
     "iopub.status.busy": "2023-11-30T04:27:53.514712Z",
     "iopub.status.idle": "2023-11-30T04:27:53.538386Z",
     "shell.execute_reply": "2023-11-30T04:27:53.537796Z",
     "shell.execute_reply.started": "2023-11-30T04:27:53.514812Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "N_NEIGHBORS_MAX = 80 if not CFG.is_test_mode else 5\n",
    "\n",
    "class Neighbors:\n",
    "    def __init__(self, \n",
    "                 name: str, \n",
    "                 pivot: pd.DataFrame, \n",
    "                 p: float, \n",
    "                 metric: str = 'minkowski', \n",
    "                 metric_params: Optional[Dict] = None, \n",
    "                 exclude_self: bool = False):\n",
    "        self.name = name\n",
    "        self.exclude_self = exclude_self\n",
    "        self.p = p\n",
    "        self.metric = metric\n",
    "        \n",
    "        if metric == 'random':\n",
    "            n_queries = len(pivot)\n",
    "            self.neighbors = np.random.randint(n_queries, size=(n_queries, N_NEIGHBORS_MAX))\n",
    "        else:\n",
    "            nn = NearestNeighbors(\n",
    "                n_neighbors=N_NEIGHBORS_MAX, \n",
    "                p=p, \n",
    "                metric=metric, \n",
    "                metric_params=metric_params\n",
    "            )\n",
    "            nn.fit(pivot)\n",
    "            _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n",
    "\n",
    "        self.columns = self.index = self.feature_values = self.feature_col = None\n",
    "\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def make_nn_feature(self, n=5, agg=np.mean) -> pd.DataFrame:\n",
    "        assert self.feature_values is not None, \"should call rearrange_feature_values beforehand\"\n",
    "\n",
    "        start = 1 if self.exclude_self else 0\n",
    "\n",
    "        pivot_aggs = pd.DataFrame(\n",
    "            agg(self.feature_values[start:n,:,:], axis=0), \n",
    "            columns=self.columns, \n",
    "            index=self.index\n",
    "        )\n",
    "\n",
    "        dst = pivot_aggs.unstack().reset_index()\n",
    "        dst.columns = ['stock_id', 'date_seconds_id', f'{self.feature_col}_nn{n}_{self.name}_{agg.__name__}']\n",
    "        return dst\n",
    "\n",
    "\n",
    "class TimeIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        feature_pivot = df.pivot('date_seconds_id', 'stock_id', feature_col)\n",
    "        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "        feature_pivot.head()\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, :] += feature_pivot.values[self.neighbors[:, i], :]\n",
    "\n",
    "        self.columns = list(feature_pivot.columns)\n",
    "        self.index = list(feature_pivot.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"time-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n",
    "\n",
    "\n",
    "class StockIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        \"\"\"stock-id based nearest neighbor features\"\"\"\n",
    "        feature_pivot = df.pivot(index='date_seconds_id', columns='stock_id', values=feature_col)\n",
    "        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, :] += feature_pivot.values[:, self.neighbors[:, i]]\n",
    "\n",
    "        self.columns = list(feature_pivot.columns)\n",
    "        self.index = list(feature_pivot.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"stock-id NN (name={self.name}, metric={self.metric}, p={self.p})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T04:27:53.539097Z",
     "iopub.status.busy": "2023-11-30T04:27:53.538978Z",
     "iopub.status.idle": "2023-11-30T04:27:53.541416Z",
     "shell.execute_reply": "2023-11-30T04:27:53.540982Z",
     "shell.execute_reply.started": "2023-11-30T04:27:53.539086Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# pivot = df_train.pivot(index='time_id', columns='stock_id', values='target')\n",
    "# pivot = pivot.fillna(pivot.mean())\n",
    "# pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "# stock_id_neighbors = []\n",
    "# stock_id_neighbors.append(StockIdNeighbors(\n",
    "#     name='stock_price_l1', \n",
    "#     pivot=minmax_scale(pivot.T), \n",
    "#     p=1, # manhattan_distance (l1)\n",
    "#     exclude_self=True\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T04:27:53.542078Z",
     "iopub.status.busy": "2023-11-30T04:27:53.541920Z",
     "iopub.status.idle": "2023-11-30T04:27:53.544295Z",
     "shell.execute_reply": "2023-11-30T04:27:53.543861Z",
     "shell.execute_reply.started": "2023-11-30T04:27:53.542068Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# stock_ids = np.array(sorted(df_train['stock_id'].unique()))\n",
    "# # for neighbor in stock_id_neighbors:\n",
    "# print(neighbor)\n",
    "# display(\n",
    "#     pd.DataFrame(\n",
    "#         stock_ids[stock_id_neighbors[0].neighbors[:,:5]], \n",
    "#         index=pd.Index(stock_ids, name='stock_id'), \n",
    "#         columns=[f'top_{i+1}' for i in range(5)]\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T04:29:28.144605Z",
     "iopub.status.busy": "2023-11-30T04:29:28.144433Z",
     "iopub.status.idle": "2023-11-30T04:29:34.196300Z",
     "shell.execute_reply": "2023-11-30T04:29:34.195719Z",
     "shell.execute_reply.started": "2023-11-30T04:29:28.144594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "      <th>top_4</th>\n",
       "      <th>top_5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>137</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>142</td>\n",
       "      <td>153</td>\n",
       "      <td>70</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>137</td>\n",
       "      <td>79</td>\n",
       "      <td>37</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>137</td>\n",
       "      <td>131</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>148</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>137</td>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>137</td>\n",
       "      <td>9</td>\n",
       "      <td>131</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>15</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          top_1  top_2  top_3  top_4  top_5\n",
       "stock_id                                   \n",
       "0             0     37    110      2     66\n",
       "1             1     79    137     37      3\n",
       "2             2    142    153     70    174\n",
       "3             3    137     79     37    109\n",
       "4             4      9    137    131     99\n",
       "...         ...    ...    ...    ...    ...\n",
       "195         195    148     37      3    123\n",
       "196         196    137     79      3     37\n",
       "197         197    137      9    131     99\n",
       "198         198     12      5     15    115\n",
       "199         199     15     49     52    115\n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['date_seconds_id'] = df['date_id'].astype(str).str.zfill(3) + df['seconds_in_bucket'].astype(str).str.zfill(3)\n",
    "pivot = df.pivot(index='date_seconds_id', columns='stock_id', values='target')\n",
    "pivot = pivot.fillna(pivot.mean())\n",
    "pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "stock_id_neighbors = []\n",
    "stock_id_neighbors.append(StockIdNeighbors(\n",
    "    name='stock_price_l1', \n",
    "    pivot=minmax_scale(pivot.T), \n",
    "    p=1, # manhattan_distance (l1)\n",
    "    exclude_self=True\n",
    "))\n",
    "\n",
    "stock_ids = np.array(sorted(df['stock_id'].unique()))\n",
    "# for neighbor in stock_id_neighbors:\n",
    "# print(neighbor)\n",
    "display(\n",
    "    pd.DataFrame(\n",
    "        stock_ids[stock_id_neighbors[0].neighbors[:,:5]], \n",
    "        index=pd.Index(stock_ids, name='stock_id'), \n",
    "        columns=[f'top_{i+1}' for i in range(5)]\n",
    "    )\n",
    ")\n",
    "\n",
    "def make_nearest_neighbor_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = df.copy()\n",
    "    feature_cols_stock = {\n",
    "        'imbalance_size': [np.mean, np.min, np.max, np.std],\n",
    "        'reference_price': [np.mean, np.min, np.max, np.std],\n",
    "        'matched_size': [np.mean],\n",
    "        'far_price': [np.mean],\n",
    "        'near_price': [np.mean],\n",
    "        'bid_price': [np.mean],\n",
    "        'ask_price': [np.mean],\n",
    "        'wap': [np.mean],\n",
    "    }\n",
    "    stock_id_neighbor_sizes = [10, 20, 40]\n",
    "    \n",
    "    ndf: Optional[pd.DataFrame] = None\n",
    "    \n",
    "    def _add_ndf(ndf: Optional[pd.DataFrame], dst: pd.DataFrame) -> pd.DataFrame:\n",
    "        if ndf is None:\n",
    "            return dst\n",
    "        else:\n",
    "            ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
    "            return ndf\n",
    "    \n",
    "    # neighbor stock_id\n",
    "    for feature_col in feature_cols_stock.keys():\n",
    "        try:\n",
    "            if feature_col not in df2.columns:\n",
    "                print(f\"column {feature_col} is skipped\")\n",
    "                continue\n",
    "        \n",
    "            if not stock_id_neighbors:\n",
    "                continue\n",
    "        \n",
    "            for nn in stock_id_neighbors:\n",
    "                nn.rearrange_feature_values(df2, feature_col)\n",
    "        \n",
    "            for agg in feature_cols_stock[feature_col]:\n",
    "                for n in stock_id_neighbor_sizes:\n",
    "                    try:\n",
    "                        for nn in stock_id_neighbors:\n",
    "                            dst = nn.make_nn_feature(n, agg)\n",
    "                            ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception as e:\n",
    "                        print('stock-id nn', e)\n",
    "                        pass\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    if ndf is not None:\n",
    "        df2 = pd.merge(df2, ndf, on=['date_seconds_id', 'stock_id'], how='left')\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Feature Generation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-30T04:27:58.319066Z",
     "iopub.status.idle": "2023-11-30T04:27:58.319219Z",
     "shell.execute_reply": "2023-11-30T04:27:58.319147Z",
     "shell.execute_reply.started": "2023-11-30T04:27:58.319139Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imbalance_features(df):\n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "    df[\"size_imbalance_bid\"] = df.eval(\"imbalance_size / bid_size\")\n",
    "    df[\"size_imbalance_ask\"] = df.eval(\"imbalance_size / ask_size\")\n",
    "    df[\"matched_size_bid_ask\"] = df.eval(\"matched_size / (bid_size+ask_size)\")\n",
    "\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "   \n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    # Calculate various statistical aggregation features\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "\n",
    "            if window == 1:\n",
    "                continue\n",
    "\n",
    "            date_ids = df['date_id'].unique()\n",
    "            agg_func = ['mean', 'sum']\n",
    "            moving_li = []\n",
    "            \n",
    "            for date in date_ids:\n",
    "                moving_li.append(\n",
    "                    df[df['date_id'] == date].groupby('stock_id')[col].rolling(window).agg(agg_func).reset_index().set_index('level_1')[agg_func]\n",
    "                )\n",
    "            df[[f'{col}_moving_average_{window}', f'{col}_moving_sum_{window}']] = pd.concat(moving_li)[agg_func]\n",
    "    \n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "\n",
    "    # --- add\n",
    "    # Calculate diff prices\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f'{c[0]}_{c[1]}_diff'] = df.eval(f'({c[0]} - {c[1]})')\n",
    "\n",
    "    return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    for key, value in global_weight_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"weight_label\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_all_features(df, feature_name=None):\n",
    "    # Select relevant columns for feature generation\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", 'time_id']]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Generate imbalance features\n",
    "    df = imbalance_features(df)\n",
    "    df = other_features(df)\n",
    "    df = make_nearest_neighbor_feature(df)\n",
    "    gc.collect()\n",
    "\n",
    "    if not feature_name:\n",
    "        feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\", 'ask_price_bid_price_diff', 'date_seconds_id']]\n",
    "    \n",
    "    return df[feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-30T04:27:58.319642Z",
     "iopub.status.idle": "2023-11-30T04:27:58.319781Z",
     "shell.execute_reply": "2023-11-30T04:27:58.319716Z",
     "shell.execute_reply.started": "2023-11-30T04:27:58.319709Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "# weights = {int(k):v for k,v in enumerate(weights)}\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "weight_label = pd.Series(le.fit_transform(weights), name='weight_label')\n",
    "weight_label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-30T04:27:58.320127Z",
     "iopub.status.idle": "2023-11-30T04:27:58.320260Z",
     "shell.execute_reply": "2023-11-30T04:27:58.320197Z",
     "shell.execute_reply.started": "2023-11-30T04:27:58.320191Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df.copy()\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-30T04:27:58.320742Z",
     "iopub.status.idle": "2023-11-30T04:27:58.320876Z",
     "shell.execute_reply": "2023-11-30T04:27:58.320813Z",
     "shell.execute_reply.started": "2023-11-30T04:27:58.320806Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_stock_id_feats = {\n",
    "    \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "    \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "    \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "    \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "    \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "    \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "}\n",
    "\n",
    "df_train = pd.merge(df_train, weight_label, left_on='stock_id', right_index=True)\n",
    "global_weight_feats = {\n",
    "    \"median_size\": df_train.groupby(\"weight_label\")[\"bid_size\"].median() + df_train.groupby(\"weight_label\")[\"ask_size\"].median(),\n",
    "    \"std_size\": df_train.groupby(\"weight_label\")[\"bid_size\"].std() + df_train.groupby(\"weight_label\")[\"ask_size\"].std(),\n",
    "    \"ptp_size\": df_train.groupby(\"weight_label\")[\"bid_size\"].max() - df_train.groupby(\"weight_label\")[\"bid_size\"].min(),\n",
    "    \"median_price\": df_train.groupby(\"weight_label\")[\"bid_price\"].median() + df_train.groupby(\"weight_label\")[\"ask_price\"].median(),\n",
    "    \"std_price\": df_train.groupby(\"weight_label\")[\"bid_price\"].std() + df_train.groupby(\"weight_label\")[\"ask_price\"].std(),\n",
    "    \"ptp_price\": df_train.groupby(\"weight_label\")[\"bid_price\"].max() - df_train.groupby(\"weight_label\")[\"ask_price\"].min(),\n",
    "}\n",
    "if CFG.TRAINING:\n",
    "    df_train_feats = generate_all_features(df_train)\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats)\n",
    "\n",
    "    feature_name = list(df_train_feats.columns)\n",
    "    print(f'Feature length = {len(feature_name)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-30T04:27:58.321394Z",
     "iopub.status.idle": "2023-11-30T04:27:58.321529Z",
     "shell.execute_reply": "2023-11-30T04:27:58.321466Z",
     "shell.execute_reply.started": "2023-11-30T04:27:58.321459Z"
    }
   },
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_columns', 200):\n",
    "#     display(\n",
    "#         pd.concat(\n",
    "#             [df_train_feats[df_train_feats['stock_id'] == 0].head(15),\n",
    "#             df_train_feats[df_train_feats['stock_id'] == 0].tail(5)]\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_feats.columns[df_train_feats.head(1000).T.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T12:58:29.523686Z",
     "iopub.status.busy": "2023-11-29T12:58:29.523185Z",
     "iopub.status.idle": "2023-11-29T14:46:26.260105Z",
     "shell.execute_reply": "2023-11-29T14:46:26.259435Z",
     "shell.execute_reply.started": "2023-11-29T12:58:29.523667Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature length = 208\n",
      "Fold 1 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 7.10311\n",
      "[200]\tvalid_0's l1: 7.05841\n",
      "[300]\tvalid_0's l1: 7.03519\n",
      "[400]\tvalid_0's l1: 7.01581\n",
      "[500]\tvalid_0's l1: 6.99921\n",
      "[600]\tvalid_0's l1: 6.98451\n",
      "[700]\tvalid_0's l1: 6.97147\n",
      "[800]\tvalid_0's l1: 6.95919\n",
      "[900]\tvalid_0's l1: 6.94828\n",
      "[1000]\tvalid_0's l1: 6.93758\n",
      "[1100]\tvalid_0's l1: 6.92785\n",
      "[1200]\tvalid_0's l1: 6.91872\n",
      "[1300]\tvalid_0's l1: 6.91055\n",
      "[1400]\tvalid_0's l1: 6.9018\n",
      "[1500]\tvalid_0's l1: 6.89316\n",
      "[1600]\tvalid_0's l1: 6.88568\n",
      "[1700]\tvalid_0's l1: 6.87838\n",
      "[1800]\tvalid_0's l1: 6.87055\n",
      "[1900]\tvalid_0's l1: 6.86365\n",
      "[2000]\tvalid_0's l1: 6.85669\n",
      "[2100]\tvalid_0's l1: 6.84975\n",
      "[2200]\tvalid_0's l1: 6.84315\n",
      "[2300]\tvalid_0's l1: 6.83618\n",
      "[2400]\tvalid_0's l1: 6.82968\n",
      "[2500]\tvalid_0's l1: 6.82283\n",
      "[2600]\tvalid_0's l1: 6.81639\n",
      "[2700]\tvalid_0's l1: 6.81\n",
      "[2800]\tvalid_0's l1: 6.80386\n",
      "[2900]\tvalid_0's l1: 6.79789\n",
      "[3000]\tvalid_0's l1: 6.79155\n",
      "[3100]\tvalid_0's l1: 6.78558\n",
      "[3200]\tvalid_0's l1: 6.77973\n",
      "[3300]\tvalid_0's l1: 6.77406\n",
      "[3400]\tvalid_0's l1: 6.76843\n",
      "[3500]\tvalid_0's l1: 6.7627\n",
      "[3600]\tvalid_0's l1: 6.757\n",
      "[3700]\tvalid_0's l1: 6.75121\n",
      "[3800]\tvalid_0's l1: 6.74558\n",
      "[3900]\tvalid_0's l1: 6.74022\n",
      "[4000]\tvalid_0's l1: 6.73487\n",
      "[4100]\tvalid_0's l1: 6.72934\n",
      "[4200]\tvalid_0's l1: 6.72378\n",
      "[4300]\tvalid_0's l1: 6.71822\n",
      "[4400]\tvalid_0's l1: 6.71319\n",
      "[4500]\tvalid_0's l1: 6.70734\n",
      "[4600]\tvalid_0's l1: 6.70266\n",
      "[4700]\tvalid_0's l1: 6.69723\n",
      "[4800]\tvalid_0's l1: 6.69225\n",
      "[4900]\tvalid_0's l1: 6.68703\n",
      "[5000]\tvalid_0's l1: 6.6818\n",
      "[5100]\tvalid_0's l1: 6.67714\n",
      "[5200]\tvalid_0's l1: 6.67208\n",
      "[5300]\tvalid_0's l1: 6.66745\n",
      "[5400]\tvalid_0's l1: 6.66266\n",
      "[5500]\tvalid_0's l1: 6.65797\n",
      "[5600]\tvalid_0's l1: 6.65332\n",
      "[5700]\tvalid_0's l1: 6.64839\n",
      "[5800]\tvalid_0's l1: 6.64356\n",
      "[5900]\tvalid_0's l1: 6.63912\n",
      "[6000]\tvalid_0's l1: 6.63484\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\tvalid_0's l1: 6.63484\n",
      "Model for fold 1 saved to /kaggle/input/optiver-trading-at-the-close/lgb_model/lgb_cv1.txt\n",
      "Fold 1 MAE: 6.634837898611561\n",
      "Fold 2 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.42579\n",
      "[200]\tvalid_0's l1: 6.39452\n",
      "[300]\tvalid_0's l1: 6.37664\n",
      "[400]\tvalid_0's l1: 6.35959\n",
      "[500]\tvalid_0's l1: 6.34435\n",
      "[600]\tvalid_0's l1: 6.33105\n",
      "[700]\tvalid_0's l1: 6.31889\n",
      "[800]\tvalid_0's l1: 6.30734\n",
      "[900]\tvalid_0's l1: 6.29692\n",
      "[1000]\tvalid_0's l1: 6.28694\n",
      "[1100]\tvalid_0's l1: 6.27734\n",
      "[1200]\tvalid_0's l1: 6.26849\n",
      "[1300]\tvalid_0's l1: 6.26042\n",
      "[1400]\tvalid_0's l1: 6.25194\n",
      "[1500]\tvalid_0's l1: 6.24416\n",
      "[1600]\tvalid_0's l1: 6.237\n",
      "[1700]\tvalid_0's l1: 6.23053\n",
      "[1800]\tvalid_0's l1: 6.22339\n",
      "[1900]\tvalid_0's l1: 6.21686\n",
      "[2000]\tvalid_0's l1: 6.21037\n",
      "[2100]\tvalid_0's l1: 6.20364\n",
      "[2200]\tvalid_0's l1: 6.19767\n",
      "[2300]\tvalid_0's l1: 6.19132\n",
      "[2400]\tvalid_0's l1: 6.18489\n",
      "[2500]\tvalid_0's l1: 6.17882\n",
      "[2600]\tvalid_0's l1: 6.17257\n",
      "[2700]\tvalid_0's l1: 6.16649\n",
      "[2800]\tvalid_0's l1: 6.16048\n",
      "[2900]\tvalid_0's l1: 6.15493\n",
      "[3000]\tvalid_0's l1: 6.14993\n",
      "[3100]\tvalid_0's l1: 6.14438\n",
      "[3200]\tvalid_0's l1: 6.13932\n",
      "[3300]\tvalid_0's l1: 6.13406\n",
      "[3400]\tvalid_0's l1: 6.12867\n",
      "[3500]\tvalid_0's l1: 6.12298\n",
      "[3600]\tvalid_0's l1: 6.11749\n",
      "[3700]\tvalid_0's l1: 6.11278\n",
      "[3800]\tvalid_0's l1: 6.10727\n",
      "[3900]\tvalid_0's l1: 6.10163\n",
      "[4000]\tvalid_0's l1: 6.09724\n",
      "[4100]\tvalid_0's l1: 6.09334\n",
      "[4200]\tvalid_0's l1: 6.09013\n",
      "[4300]\tvalid_0's l1: 6.08716\n",
      "[4400]\tvalid_0's l1: 6.08427\n",
      "[4500]\tvalid_0's l1: 6.08167\n",
      "[4600]\tvalid_0's l1: 6.07887\n",
      "[4700]\tvalid_0's l1: 6.07662\n",
      "[4800]\tvalid_0's l1: 6.07392\n",
      "[4900]\tvalid_0's l1: 6.07163\n",
      "[5000]\tvalid_0's l1: 6.06903\n",
      "[5100]\tvalid_0's l1: 6.06676\n",
      "[5200]\tvalid_0's l1: 6.06389\n",
      "[5300]\tvalid_0's l1: 6.06109\n",
      "[5400]\tvalid_0's l1: 6.0586\n",
      "[5500]\tvalid_0's l1: 6.05595\n",
      "[5600]\tvalid_0's l1: 6.05319\n",
      "[5700]\tvalid_0's l1: 6.05052\n",
      "[5800]\tvalid_0's l1: 6.04786\n",
      "[5900]\tvalid_0's l1: 6.04491\n",
      "[6000]\tvalid_0's l1: 6.04211\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\tvalid_0's l1: 6.04211\n",
      "Model for fold 2 saved to /kaggle/input/optiver-trading-at-the-close/lgb_model/lgb_cv2.txt\n",
      "Fold 2 MAE: 6.042110458853618\n",
      "Fold 3 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.33758\n",
      "[200]\tvalid_0's l1: 6.30334\n",
      "[300]\tvalid_0's l1: 6.28175\n",
      "[400]\tvalid_0's l1: 6.26217\n",
      "[500]\tvalid_0's l1: 6.245\n",
      "[600]\tvalid_0's l1: 6.22988\n",
      "[700]\tvalid_0's l1: 6.21531\n",
      "[800]\tvalid_0's l1: 6.20214\n",
      "[900]\tvalid_0's l1: 6.18964\n",
      "[1000]\tvalid_0's l1: 6.17834\n",
      "[1100]\tvalid_0's l1: 6.16728\n",
      "[1200]\tvalid_0's l1: 6.15631\n",
      "[1300]\tvalid_0's l1: 6.14649\n",
      "[1400]\tvalid_0's l1: 6.13714\n",
      "[1500]\tvalid_0's l1: 6.12857\n",
      "[1600]\tvalid_0's l1: 6.12013\n",
      "[1700]\tvalid_0's l1: 6.11193\n",
      "[1800]\tvalid_0's l1: 6.10381\n",
      "[1900]\tvalid_0's l1: 6.09586\n",
      "[2000]\tvalid_0's l1: 6.08787\n",
      "[2100]\tvalid_0's l1: 6.08005\n",
      "[2200]\tvalid_0's l1: 6.07309\n",
      "[2300]\tvalid_0's l1: 6.06506\n",
      "[2400]\tvalid_0's l1: 6.05721\n",
      "[2500]\tvalid_0's l1: 6.0498\n",
      "[2600]\tvalid_0's l1: 6.04244\n",
      "[2700]\tvalid_0's l1: 6.03555\n",
      "[2800]\tvalid_0's l1: 6.02917\n",
      "[2900]\tvalid_0's l1: 6.02333\n",
      "[3000]\tvalid_0's l1: 6.01675\n",
      "[3100]\tvalid_0's l1: 6.01041\n",
      "[3200]\tvalid_0's l1: 6.00398\n",
      "[3300]\tvalid_0's l1: 5.99779\n",
      "[3400]\tvalid_0's l1: 5.9917\n",
      "[3500]\tvalid_0's l1: 5.98552\n",
      "[3600]\tvalid_0's l1: 5.97957\n",
      "[3700]\tvalid_0's l1: 5.97348\n",
      "[3800]\tvalid_0's l1: 5.96759\n",
      "[3900]\tvalid_0's l1: 5.96212\n",
      "[4000]\tvalid_0's l1: 5.95733\n",
      "[4100]\tvalid_0's l1: 5.95227\n",
      "[4200]\tvalid_0's l1: 5.94764\n",
      "[4300]\tvalid_0's l1: 5.94326\n",
      "[4400]\tvalid_0's l1: 5.93906\n",
      "[4500]\tvalid_0's l1: 5.93478\n",
      "[4600]\tvalid_0's l1: 5.93077\n",
      "[4700]\tvalid_0's l1: 5.92708\n",
      "[4800]\tvalid_0's l1: 5.92306\n",
      "[4900]\tvalid_0's l1: 5.91889\n",
      "[5000]\tvalid_0's l1: 5.9142\n",
      "[5100]\tvalid_0's l1: 5.90955\n",
      "[5200]\tvalid_0's l1: 5.90497\n",
      "[5300]\tvalid_0's l1: 5.90123\n",
      "[5400]\tvalid_0's l1: 5.89706\n",
      "[5500]\tvalid_0's l1: 5.89305\n",
      "[5600]\tvalid_0's l1: 5.88889\n",
      "[5700]\tvalid_0's l1: 5.88496\n",
      "[5800]\tvalid_0's l1: 5.88185\n",
      "[5900]\tvalid_0's l1: 5.87845\n",
      "[6000]\tvalid_0's l1: 5.8751\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\tvalid_0's l1: 5.8751\n",
      "Model for fold 3 saved to /kaggle/input/optiver-trading-at-the-close/lgb_model/lgb_cv3.txt\n",
      "Fold 3 MAE: 5.875095294501947\n",
      "Fold 4 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.97603\n",
      "[200]\tvalid_0's l1: 5.93797\n",
      "[300]\tvalid_0's l1: 5.91186\n",
      "[400]\tvalid_0's l1: 5.88832\n",
      "[500]\tvalid_0's l1: 5.86794\n",
      "[600]\tvalid_0's l1: 5.84871\n",
      "[700]\tvalid_0's l1: 5.83\n",
      "[800]\tvalid_0's l1: 5.81313\n",
      "[900]\tvalid_0's l1: 5.79771\n",
      "[1000]\tvalid_0's l1: 5.7827\n",
      "[1100]\tvalid_0's l1: 5.768\n",
      "[1200]\tvalid_0's l1: 5.75471\n",
      "[1300]\tvalid_0's l1: 5.74275\n",
      "[1400]\tvalid_0's l1: 5.73081\n",
      "[1500]\tvalid_0's l1: 5.71973\n",
      "[1600]\tvalid_0's l1: 5.70838\n",
      "[1700]\tvalid_0's l1: 5.69718\n",
      "[1800]\tvalid_0's l1: 5.68693\n",
      "[1900]\tvalid_0's l1: 5.67598\n",
      "[2000]\tvalid_0's l1: 5.666\n",
      "[2100]\tvalid_0's l1: 5.6559\n",
      "[2200]\tvalid_0's l1: 5.64697\n",
      "[2300]\tvalid_0's l1: 5.63631\n",
      "[2400]\tvalid_0's l1: 5.6267\n",
      "[2500]\tvalid_0's l1: 5.6175\n",
      "[2600]\tvalid_0's l1: 5.60896\n",
      "[2700]\tvalid_0's l1: 5.60171\n",
      "[2800]\tvalid_0's l1: 5.59335\n",
      "[2900]\tvalid_0's l1: 5.58594\n",
      "[3000]\tvalid_0's l1: 5.57805\n",
      "[3100]\tvalid_0's l1: 5.57021\n",
      "[3200]\tvalid_0's l1: 5.56273\n",
      "[3300]\tvalid_0's l1: 5.55573\n",
      "[3400]\tvalid_0's l1: 5.54869\n",
      "[3500]\tvalid_0's l1: 5.54244\n",
      "[3600]\tvalid_0's l1: 5.53675\n",
      "[3700]\tvalid_0's l1: 5.53114\n",
      "[3800]\tvalid_0's l1: 5.52539\n",
      "[3900]\tvalid_0's l1: 5.52066\n",
      "[4000]\tvalid_0's l1: 5.51564\n",
      "[4100]\tvalid_0's l1: 5.51078\n",
      "[4200]\tvalid_0's l1: 5.50653\n",
      "[4300]\tvalid_0's l1: 5.50219\n",
      "[4400]\tvalid_0's l1: 5.49737\n",
      "[4500]\tvalid_0's l1: 5.49282\n",
      "[4600]\tvalid_0's l1: 5.48915\n",
      "[4700]\tvalid_0's l1: 5.48563\n",
      "[4800]\tvalid_0's l1: 5.48178\n",
      "[4900]\tvalid_0's l1: 5.47834\n",
      "[5000]\tvalid_0's l1: 5.47484\n",
      "[5100]\tvalid_0's l1: 5.4709\n",
      "[5200]\tvalid_0's l1: 5.4668\n",
      "[5300]\tvalid_0's l1: 5.46435\n",
      "[5400]\tvalid_0's l1: 5.46193\n",
      "[5500]\tvalid_0's l1: 5.45986\n",
      "[5600]\tvalid_0's l1: 5.45794\n",
      "[5700]\tvalid_0's l1: 5.4562\n",
      "[5800]\tvalid_0's l1: 5.45475\n",
      "[5900]\tvalid_0's l1: 5.45331\n",
      "[6000]\tvalid_0's l1: 5.4521\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\tvalid_0's l1: 5.4521\n",
      "Model for fold 4 saved to /kaggle/input/optiver-trading-at-the-close/lgb_model/lgb_cv4.txt\n",
      "Fold 4 MAE: 5.452098288328964\n",
      "Fold 5 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 4.83913\n",
      "[200]\tvalid_0's l1: 4.823\n",
      "[300]\tvalid_0's l1: 4.81652\n",
      "[400]\tvalid_0's l1: 4.81418\n",
      "[500]\tvalid_0's l1: 4.81408\n",
      "Early stopping, best iteration is:\n",
      "[479]\tvalid_0's l1: 4.81353\n",
      "Model for fold 5 saved to /kaggle/input/optiver-trading-at-the-close/lgb_model/lgb_cv5.txt\n",
      "Fold 5 MAE: 4.813533386996959\n",
      "Training final model with average best iteration: 4895\n",
      "Final model saved to /kaggle/input/optiver-trading-at-the-close/lgb_model/lgb_fin.txt\n",
      "Average MAE across all folds: 5.763535065458609\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import gc\n",
    "\n",
    "if CFG.TRAINING and 'LGBM' in CFG.methods:\n",
    "    lgb_params = {\n",
    "        \"objective\": \"mae\",\n",
    "        \"n_estimators\": 6000 if not CFG.is_test_mode else 500,\n",
    "        \"num_leaves\": 256,\n",
    "        \"subsample\": 0.6,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"learning_rate\": 0.00871,\n",
    "        'max_depth': 11,\n",
    "        \"n_jobs\": 4,\n",
    "        \"device\": 'gpu' if CFG.is_gpu else 'cpu',\n",
    "        \"verbosity\": -1,\n",
    "        \"importance_type\": \"gain\",\n",
    "        'seed': CFG.state,\n",
    "    }\n",
    "    feature_name = list(df_train_feats.columns)\n",
    "    print(f\"Feature length = {len(feature_name)}\")\n",
    "    \n",
    "    num_folds = 5\n",
    "    fold_size = 480 // num_folds\n",
    "    gap = 5\n",
    "    \n",
    "    lgb_models = []\n",
    "    scores = []\n",
    "    \n",
    "    model_save_path = f'{CFG.model_path}lgb_model' \n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "    \n",
    "    date_ids = df_train['date_id'].values\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size\n",
    "        if i < num_folds - 1:  # No need to purge after the last fold\n",
    "            purged_start = end - 2\n",
    "            purged_end = end + gap + 2\n",
    "            train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n",
    "        else:\n",
    "            train_indices = (date_ids >= start) & (date_ids < end)\n",
    "        \n",
    "        test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n",
    "        \n",
    "        df_fold_train = df_train_feats[train_indices]\n",
    "        df_fold_train_target = df_train['target'][train_indices]\n",
    "        df_fold_valid = df_train_feats[test_indices]\n",
    "        df_fold_valid_target = df_train['target'][test_indices]\n",
    "    \n",
    "        print(f\"Fold {i+1} Model Training\")\n",
    "        \n",
    "        # Train a LightGBM model for the current fold\n",
    "        lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "        lgb_model.fit(\n",
    "            df_fold_train[feature_name],\n",
    "            df_fold_train_target,\n",
    "            eval_set=[(df_fold_valid[feature_name], df_fold_valid_target)],\n",
    "            callbacks=[\n",
    "                lgb.callback.early_stopping(stopping_rounds=100),\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "        lgb_models.append(lgb_model)\n",
    "        # Save the model to a file\n",
    "        model_filename = os.path.join(model_save_path, f'lgb_cv{i+1}.txt')\n",
    "        lgb_model.booster_.save_model(model_filename, importance_type='gain')\n",
    "        print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "    \n",
    "        # Evaluate model performance on the validation set\n",
    "        fold_predictions = lgb_model.predict(df_fold_valid[feature_name])\n",
    "        fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n",
    "        scores.append(fold_score)\n",
    "        print(f\"Fold {i+1} MAE: {fold_score}\")\n",
    "    \n",
    "        # Free up memory by deleting fold specific variables\n",
    "        del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n",
    "        gc.collect()\n",
    "    \n",
    "    # Calculate the average best iteration from all regular folds\n",
    "    average_best_iteration = int(np.mean([model.best_iteration_ for model in lgb_models]))\n",
    "    \n",
    "    # Update the lgb_params with the average best iteration\n",
    "    final_model_params = lgb_params.copy()\n",
    "    final_model_params['n_estimators'] = average_best_iteration\n",
    "    \n",
    "    print(f\"Training final model with average best iteration: {average_best_iteration}\")\n",
    "    \n",
    "    # Train the final model on the entire dataset\n",
    "    final_model = lgb.LGBMRegressor(**final_model_params)\n",
    "    final_model.fit(\n",
    "        df_train_feats[feature_name],\n",
    "        df_train['target'],\n",
    "        callbacks=[\n",
    "            lgb.callback.log_evaluation(period=100),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # Append the final model to the list of models\n",
    "    lgb_models.append(final_model)\n",
    "    \n",
    "    # Save the final model to a file\n",
    "    final_model_filename = os.path.join(model_save_path, 'lgb_fin.txt')\n",
    "    final_model.booster_.save_model(final_model_filename, importance_type='gain')\n",
    "    print(f\"Final model saved to {final_model_filename}\")\n",
    "    \n",
    "    # Now 'models' holds the trained models for each fold and 'scores' holds the validation scores\n",
    "    print(f\"Average MAE across all folds: {np.mean(scores)}\")\n",
    "\n",
    "    os.makedirs(f'{model_save_path}/scores', exist_ok=True)\n",
    "    scores.insert(0, lgb_params)\n",
    "    scores.insert(1, len(feature_name))\n",
    "    scores.insert(2, feature_name)\n",
    "    with open(f'{model_save_path}/scores/lgbm{datetime.datetime.now()}_{len(feature_name)}.txt', 'w') as f:\n",
    "        print(*scores, file=f, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.TRAINING and 'CBT' in CFG.methods:\n",
    "    cbt_params = {\n",
    "        'task_type'           : 'GPU' if CFG.is_gpu else 'CPU',\n",
    "        'objective'           : 'MAE',\n",
    "        'eval_metric'         : 'MAE',\n",
    "        'bagging_temperature' : 0.5,\n",
    "    #     'colsample_bylevel'   : 0.7,\n",
    "        'iterations'          : 500 if not CFG.is_test_mode else 100,\n",
    "        'early_stopping_rounds' : 50 if not CFG.is_test_mode else 10,\n",
    "        'learning_rate'       : 0.065,\n",
    "        'max_depth'           : 7,\n",
    "        'l2_leaf_reg'         : 1.5,\n",
    "        'min_data_in_leaf'    : 1000,\n",
    "        'random_strength'     : 0.65, \n",
    "        'verbose'             : 0,\n",
    "        'use_best_model'      : True,\n",
    "        'random_seed'         : CFG.state,\n",
    "    }\n",
    "    feature_name = list(df_train_feats.columns)\n",
    "    print(f\"Feature length = {len(feature_name)}\")\n",
    "    \n",
    "    num_folds = 5\n",
    "    fold_size = 480 // num_folds\n",
    "    gap = 5\n",
    "    \n",
    "    cbt_models = []\n",
    "    scores = []\n",
    "    \n",
    "    model_save_path = f'{CFG.model_path}cbt_model' \n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "    \n",
    "    date_ids = df_train['date_id'].values\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size\n",
    "        if i < num_folds - 1:  # No need to purge after the last fold\n",
    "            purged_start = end - 2\n",
    "            purged_end = end + gap + 2\n",
    "            train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n",
    "        else:\n",
    "            train_indices = (date_ids >= start) & (date_ids < end)\n",
    "        \n",
    "        test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n",
    "        \n",
    "        df_fold_train = df_train_feats[train_indices]\n",
    "        df_fold_train_target = df_train['target'][train_indices]\n",
    "        df_fold_valid = df_train_feats[test_indices]\n",
    "        df_fold_valid_target = df_train['target'][test_indices]\n",
    "        cbt_train = cbt.Pool(df_fold_train, df_fold_train_target)\n",
    "        cbt_valid = cbt.Pool(df_fold_valid, df_fold_valid_target)\n",
    "    \n",
    "        print(f\"Fold {i+1} Model Training\")\n",
    "        \n",
    "        # Train a LightGBM model for the current fold\n",
    "        cbt_model = cbt.CatBoostRegressor(**cbt_params)\n",
    "        cbt_model.fit(\n",
    "            cbt_train,\n",
    "            eval_set=[cbt_valid],\n",
    "        )\n",
    "    \n",
    "        cbt_models.append(cbt_model)\n",
    "        # Save the model to a file\n",
    "        model_filename = os.path.join(model_save_path, f'cbt_cv{i+1}.cbm')\n",
    "        cbt_model.save_model(model_filename)\n",
    "        print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "    \n",
    "        # Evaluate model performance on the validation set\n",
    "        fold_predictions = cbt_model.predict(df_fold_valid[feature_name])\n",
    "        fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n",
    "        scores.append(fold_score)\n",
    "        print(f\"Fold {i+1} MAE: {fold_score}\")\n",
    "    \n",
    "        # Free up memory by deleting fold specific variables\n",
    "        del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n",
    "        gc.collect()\n",
    "    \n",
    "    # Calculate the average best iteration from all regular folds\n",
    "    average_best_iteration = int(np.mean([model.get_best_iteration() for model in cbt_models]))\n",
    "    \n",
    "    # Update the lgb_params with the average best iteration\n",
    "    final_model_params = cbt_params.copy()\n",
    "    final_model_params['iterations'] = average_best_iteration\n",
    "    final_model_params['use_best_model'] = False\n",
    "    \n",
    "    print(f\"Training final model with average best iteration: {average_best_iteration}\")\n",
    "    \n",
    "    # Train the final model on the entire dataset\n",
    "    final_model = cbt.CatBoostRegressor(**final_model_params)\n",
    "    final_model.fit(\n",
    "        df_train_feats[feature_name],\n",
    "        df_train['target'],\n",
    "    )\n",
    "    \n",
    "    # Append the final model to the list of models\n",
    "    cbt_models.append(final_model)\n",
    "    \n",
    "    # Save the final model to a file\n",
    "    final_model_filename = os.path.join(model_save_path, 'cbt_fin.cbm')\n",
    "    final_model.save_model(final_model_filename)\n",
    "    print(f\"Final model saved to {final_model_filename}\")\n",
    "    \n",
    "    # Now 'models' holds the trained models for each fold and 'scores' holds the validation scores\n",
    "    print(f\"Average MAE across all folds: {np.mean(scores)}\")\n",
    "\n",
    "    os.makedirs(f'{model_save_path}/scores', exist_ok=True)\n",
    "    scores.insert(0, cbt_params)\n",
    "    scores.insert(1, len(feature_name))\n",
    "    scores.insert(2, feature_name)\n",
    "    np.savetxt(f'{model_save_path}/scores/cbt{datetime.datetime.now()}_{len(feature_name)}.txt', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "if CFG.TRAINING and 'XGB' in CFG.methods:\n",
    "    params = {\n",
    "        'task_type'           : 'GPU' if CFG.is_gpu else 'CPU',\n",
    "        'objective'           : 'MAE',\n",
    "        'eval_metric'         : 'MAE',\n",
    "        'bagging_temperature' : 0.5,\n",
    "    #     'colsample_bylevel'   : 0.7,\n",
    "        'iterations'          : 500 if not CFG.is_test_mode else 100,\n",
    "        'early_stopping_rounds' : 50 if not CFG.is_test_mode else 10,\n",
    "        'learning_rate'       : 0.065,\n",
    "        'max_depth'           : 7,\n",
    "        'l2_leaf_reg'         : 1.5,\n",
    "        'min_data_in_leaf'    : 1000,\n",
    "        'random_strength'     : 0.65, \n",
    "        'verbose'             : 0,\n",
    "        'use_best_model'      : True,\n",
    "        'random_seed'         : CFG.state,\n",
    "    }\n",
    "    feature_name = list(df_train_feats.columns)\n",
    "    print(f\"Feature length = {len(feature_name)}\")\n",
    "    \n",
    "    num_folds = 5\n",
    "    fold_size = 480 // num_folds\n",
    "    gap = 5\n",
    "    \n",
    "    xgb_models = []\n",
    "    scores = []\n",
    "    \n",
    "    model_save_path = f'{CFG.model_path}xgb_model' \n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "    \n",
    "    date_ids = df_train['date_id'].values\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size\n",
    "        if i < num_folds - 1:  # No need to purge after the last fold\n",
    "            purged_start = end - 2\n",
    "            purged_end = end + gap + 2\n",
    "            train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n",
    "        else:\n",
    "            train_indices = (date_ids >= start) & (date_ids < end)\n",
    "        \n",
    "        test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n",
    "        \n",
    "        df_fold_train = df_train_feats[train_indices]\n",
    "        df_fold_train_target = df_train['target'][train_indices]\n",
    "        df_fold_valid = df_train_feats[test_indices]\n",
    "        df_fold_valid_target = df_train['target'][test_indices]\n",
    "    \n",
    "        print(f\"Fold {i+1} Model Training\")\n",
    "        \n",
    "        # Train a LightGBM model for the current fold\n",
    "        model = xgb.XGBoostRegressor(**params)\n",
    "        model.fit(\n",
    "        )\n",
    "        \n",
    "        xgb_models.append(model)\n",
    "        # Save the model to a file\n",
    "        model_filename = os.path.join(model_save_path, f'xgb_cv{i+1}.txt')\n",
    "        model.save_model(model_filename)\n",
    "        print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "    \n",
    "        # Evaluate model performance on the validation set\n",
    "        fold_predictions = model.predict(df_fold_valid[feature_name])\n",
    "        fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n",
    "        scores.append(fold_score)\n",
    "        print(f\"Fold {i+1} MAE: {fold_score}\")\n",
    "    \n",
    "        # Free up memory by deleting fold specific variables\n",
    "        del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n",
    "        gc.collect()\n",
    "    \n",
    "    # Calculate the average best iteration from all regular folds\n",
    "    average_best_iteration = int(np.mean([model.get_best_iteration() for model in xgb_models]))\n",
    "    \n",
    "    # Update the lgb_params with the average best iteration\n",
    "    final_model_params = params.copy()\n",
    "    final_model_params['iterations'] = average_best_iteration\n",
    "    final_model_params['use_best_model'] = False\n",
    "    \n",
    "    print(f\"Training final model with average best iteration: {average_best_iteration}\")\n",
    "    \n",
    "    # Train the final model on the entire dataset\n",
    "    final_model = xgb.XGBoostRegressor(**final_model_params)\n",
    "    final_model.fit(\n",
    "        df_train_feats[feature_name],\n",
    "        df_train['target'],\n",
    "    )\n",
    "    \n",
    "    # Append the final model to the list of models\n",
    "    xgb_models.append(final_model)\n",
    "    \n",
    "    # Save the final model to a file\n",
    "    final_model_filename = os.path.join(model_save_path, 'xgb_fin.cbm')\n",
    "    final_model.save_model(final_model_filename)\n",
    "    print(f\"Final model saved to {final_model_filename}\")\n",
    "    \n",
    "    # Now 'models' holds the trained models for each fold and 'scores' holds the validation scores\n",
    "    print(f\"Average MAE across all folds: {np.mean(scores)}\")\n",
    "\n",
    "    os.makedirs(f'{model_save_path}/scores', exist_ok=True)\n",
    "    scores.insert(0, params)\n",
    "    scores.insert(1, len(feature_name))\n",
    "    scores.insert(2, feature_name)\n",
    "    np.savetxt(f'{model_save_path}/scores/xgb{datetime.datetime.now()}_{len(feature_name)}.txt', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out\n",
    "\n",
    "if CFG.INFERENCE:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    counter = 0\n",
    "    y_min, y_max = -64, 64\n",
    "    qps, predictions = [], []\n",
    "    cache = pd.DataFrame()\n",
    "    \n",
    "    lgb_models = [\n",
    "#         lgb.Booster(model_file='/kaggle/input/optiver-lgbm/lgb_model/lgb_cv1.txt'),\n",
    "#         lgb.Booster(model_file='/kaggle/input/optiver-lgbm/lgb_model/lgb_cv2.txt'),\n",
    "        lgb.Booster(model_file='/kaggle/input/optiver-lgbm/lgb_model/lgb_cv3.txt'),\n",
    "        lgb.Booster(model_file='/kaggle/input/optiver-lgbm/lgb_model/lgb_cv4.txt'),\n",
    "        lgb.Booster(model_file='/kaggle/input/optiver-lgbm/lgb_model/lgb_cv5.txt'),\n",
    "        lgb.Booster(model_file='/kaggle/input/optiver-lgbm/lgb_model/lgb_fin.txt'),\n",
    "    ]\n",
    "    cbt_models = [\n",
    "#         cbt.CatBoostRegressor().load_model('/kaggle/input/optiver-catboost/cbt_model/cbt_cv1.cbm'),\n",
    "#         cbt.CatBoostRegressor().load_model('/kaggle/input/optiver-catboost/cbt_model/cbt_cv2.cbm'),\n",
    "#         cbt.CatBoostRegressor().load_model('/kaggle/input/optiver-catboost/cbt_model/cbt_cv3.cbm'),\n",
    "#         cbt.CatBoostRegressor().load_model('/kaggle/input/optiver-catboost/cbt_model/cbt_cv4.cbm'),\n",
    "        cbt.CatBoostRegressor().load_model('/kaggle/input/optiver-catboost/cbt_model/cbt_cv5.cbm'),\n",
    "        cbt.CatBoostRegressor().load_model('/kaggle/input/optiver-catboost/cbt_model/cbt_fin.cbm'),\n",
    "    ]\n",
    "    # Weights for each fold model\n",
    "    lgb_model_weights = [0.1, 0.2, 0.3, 0.4]\n",
    "    cbt_model_weights = [0.4, 0.6]\n",
    "#     lgb_model_weights = [1/len(lgb_models)] * len(lgb_models)\n",
    "#     cbt_model_weights = [1/len(cbt_models)] * len(cbt_models)\n",
    "    \n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        now_time = time.time()\n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "        feat = generate_all_features(cache)[-len(test):]\n",
    "\n",
    "        if test.currently_scored.iloc[0] == False:\n",
    "            sample_prediction['target'] = 0\n",
    "            env.predict(sample_prediction)\n",
    "            counter += 1\n",
    "            qps.append(time.time() - now_time)\n",
    "            if counter % 10 == 0:\n",
    "                print(counter, 'qps:', np.mean(qps))\n",
    "            continue\n",
    "                    \n",
    "        feat = feat.drop(columns=['currently_scored'])\n",
    "\n",
    "        # Generate predictions for each model and calculate the weighted average\n",
    "        lgb_predictions = np.zeros(len(test))\n",
    "        for model, weight in zip(lgb_models, lgb_model_weights):\n",
    "            lgb_predictions += weight * model.predict(feat)\n",
    "\n",
    "        lgb_predictions = zero_sum(lgb_predictions, test['bid_size']+test['ask_size'])\n",
    "        clipped_predictions = np.clip(lgb_predictions, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "        # CatBoost\n",
    "        cbt_predictions = np.zeros(len(test))\n",
    "        for model, weight in zip(cbt_models, cbt_model_weights):\n",
    "            cbt_predictions += weight * model.predict(feat)\n",
    "\n",
    "        cbt_predictions = zero_sum(cbt_predictions, test['bid_size']+test['ask_size'])\n",
    "        clipped_predictions = np.clip(cbt_predictions, y_min, y_max)\n",
    "        sample_prediction['target'] = 0.4 * clipped_predictions + 0.6 * sample_prediction['target']\n",
    "\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
