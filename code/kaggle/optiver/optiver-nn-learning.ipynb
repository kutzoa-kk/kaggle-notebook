{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46e1da3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-17T04:17:10.691776Z",
     "iopub.status.busy": "2023-11-17T04:17:10.691087Z",
     "iopub.status.idle": "2023-11-17T04:17:10.696114Z",
     "shell.execute_reply": "2023-11-17T04:17:10.695449Z",
     "shell.execute_reply.started": "2023-11-17T04:17:10.691754Z"
    },
    "papermill": {
     "duration": 4.246513,
     "end_time": "2023-11-08T11:55:14.496627",
     "exception": false,
     "start_time": "2023-11-08T11:55:10.250114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from itertools import combinations\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings  # Handling warnings\n",
    "from warnings import simplefilter  # Simplifying warning handling\n",
    "\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3044460f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:22:16.400504Z",
     "iopub.status.busy": "2023-11-17T04:22:16.400218Z",
     "iopub.status.idle": "2023-11-17T04:22:16.406702Z",
     "shell.execute_reply": "2023-11-17T04:22:16.406032Z",
     "shell.execute_reply.started": "2023-11-17T04:22:16.400481Z"
    },
    "papermill": {
     "duration": 0.084256,
     "end_time": "2023-11-08T11:55:14.616085",
     "exception": false,
     "start_time": "2023-11-08T11:55:14.531829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration class\n",
    "class CFG:\n",
    "    \"\"\"\n",
    "    Configuration class for parameters and CV strategy for tuning and training\n",
    "    Please use caps lock capital letters while filling in parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data preparation\n",
    "    version_nb         = 2\n",
    "    is_gpu             = True\n",
    "    device             = torch.device('cuda' if torch.cuda.is_available() and is_gpu else 'cpu')\n",
    "    state              = 42\n",
    "    \n",
    "    is_test_mode       = False\n",
    "    test_mode_frac     = 10\n",
    "    is_offline         = True\n",
    "    \n",
    "    num_workers        = 4\n",
    "    target             = 'target'    \n",
    "    path               = '/kaggle/input/optiver-trading-at-the-close'\n",
    "    train_path         = f'{path}/train.csv'\n",
    "    test_path          = f'{path}/example_test_files/test.csv'\n",
    "    model_path         = f'' if not is_offline else f'{path}/'\n",
    "    \n",
    "    TRAINING           = True\n",
    "    INFERENCE          = False\n",
    "    \n",
    "    methods            = ['TABNET',]\n",
    "#     methods            = ['MLP', 'CNN', 'TABNET']\n",
    "\n",
    "\n",
    "    # Model Training:-\n",
    "#     methods            = [\"LGBMR\", \"CBR\", \"HGBR\"]\n",
    "#     ML                 = \"N\"\n",
    "#     n_splits           = 5\n",
    "#     n_repeats          = 1\n",
    "#     nbrnd_erly_stp     = 100 \n",
    "#     mdlcv_mthd         = 'SKF'\n",
    "    \n",
    "#     # Ensemble:-    \n",
    "#     ensemble_req       = \"Y\"\n",
    "#     enscv_mthd         = \"SKF\"\n",
    "#     metric_obj         = 'minimize'\n",
    "#     ntrials            = 10 if test_req == \"Y\" else 200\n",
    "#     ens_weights        = [0.54, 0.44, 0.02]\n",
    "    \n",
    "#     # Global variables for plotting:-\n",
    "#     grid_specs = {'visible': True, 'which': 'both', 'linestyle': '--', \n",
    "#                   'color': 'lightgrey', 'linewidth': 0.75\n",
    "#                  }\n",
    "#     title_specs = {'fontsize': 9, 'fontweight': 'bold', 'color': 'tab:blue'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c99411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:17:20.869445Z",
     "iopub.status.busy": "2023-11-17T04:17:20.869163Z",
     "iopub.status.idle": "2023-11-17T04:17:26.687496Z",
     "shell.execute_reply": "2023-11-17T04:17:26.686870Z",
     "shell.execute_reply.started": "2023-11-17T04:17:20.869424Z"
    },
    "papermill": {
     "duration": 17.933843,
     "end_time": "2023-11-08T11:55:32.558970",
     "exception": false,
     "start_time": "2023-11-08T11:55:14.625127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5237892, 17)\n"
     ]
    }
   ],
   "source": [
    "# 📂 Read the dataset from a CSV file using Pandas\n",
    "df = pd.read_csv(CFG.train_path)\n",
    "if CFG.is_test_mode:\n",
    "    df = df[df['stock_id'] < 10]\n",
    "\n",
    "# 🧹 Remove rows with missing values in the \"target\" column\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "\n",
    "# 🔁 Reset the index of the DataFrame and apply the changes in place\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 📏 Get the shape of the DataFrame (number of rows and columns)\n",
    "df_shape = df.shape\n",
    "print(df_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23da1e36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:17:31.880873Z",
     "iopub.status.busy": "2023-11-17T04:17:31.880593Z",
     "iopub.status.idle": "2023-11-17T04:17:31.887099Z",
     "shell.execute_reply": "2023-11-17T04:17:31.886646Z",
     "shell.execute_reply.started": "2023-11-17T04:17:31.880852Z"
    },
    "papermill": {
     "duration": 0.026604,
     "end_time": "2023-11-08T11:55:32.594202",
     "exception": false,
     "start_time": "2023-11-08T11:55:32.567598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 🧹 Function to reduce memory usage of a Pandas DataFrame\n",
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 📏 Calculate the initial memory usage of the DataFrame\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    # 🔄 Iterate through each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        # Check if the column's data type is not 'object' (i.e., numeric)\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            # Check if the column's data type is an integer\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                # Check if the column's data type is a float\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    # ℹ️ Provide memory optimization information if 'verbose' is True\n",
    "    if verbose:\n",
    "        print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    # 🔄 Return the DataFrame with optimized memory usage\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165f2781",
   "metadata": {
    "papermill": {
     "duration": 0.008297,
     "end_time": "2023-11-08T11:55:32.611243",
     "exception": false,
     "start_time": "2023-11-08T11:55:32.602946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c954d038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:17:33.066869Z",
     "iopub.status.busy": "2023-11-17T04:17:33.066534Z",
     "iopub.status.idle": "2023-11-17T04:17:33.358064Z",
     "shell.execute_reply": "2023-11-17T04:17:33.357385Z",
     "shell.execute_reply.started": "2023-11-17T04:17:33.066848Z"
    },
    "papermill": {
     "duration": 0.742251,
     "end_time": "2023-11-08T11:55:33.388267",
     "exception": false,
     "start_time": "2023-11-08T11:55:32.646016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 🏎️ Import Numba for just-in-time (JIT) compilation and parallel processing\n",
    "from numba import njit, prange\n",
    "\n",
    "# 📊 Function to compute triplet imbalance in parallel using Numba\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    # 🔁 Loop through all combinations of triplets\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        \n",
    "        # 🔁 Loop through rows of the DataFrame\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            # 🚫 Prevent division by zero\n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "# 📈 Function to calculate triplet imbalance for given price data and a DataFrame\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance using the Numba-optimized function\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features\n",
    "\n",
    "# 📊 Function to generate imbalance features\n",
    "def imbalance_features(df):\n",
    "    if CFG.is_gpu:\n",
    "        import cudf\n",
    "        df = cudf.from_pandas(df)\n",
    "    \n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "\n",
    "    # V1 features\n",
    "    # Calculate various features using Pandas eval function\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "    \n",
    "    # Create features for pairwise price imbalances\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "        \n",
    "    # V2 features\n",
    "    # Calculate additional features\n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    # Calculate various statistical aggregation features\n",
    "#     for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "#         df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "#         df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "    # V3 features\n",
    "    # Calculate shifted and return features for specific columns\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "    if CFG.is_gpu:\n",
    "        df = df.to_pandas()\n",
    "    # Replace infinite values with 0\n",
    "    return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "def numba_imb_features(df):\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    \n",
    "    # for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "    #     df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "    #     df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "    # Calculate triplet imbalance features using the Numba-optimized function\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "    return df\n",
    "\n",
    "# 📅 Function to generate time and stock-related features\n",
    "def other_features(df, global_stock_id_feats=[]):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n",
    "\n",
    "    # Map global features to the DataFrame\n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "# 🚀 Function to generate all features by combining imbalance and other features\n",
    "def generate_all_features(df):\n",
    "    # Select relevant columns for feature generation\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Generate imbalance features\n",
    "    df = imbalance_features(df)\n",
    "    df = numba_imb_features(df)\n",
    "    # Generate time and stock-related features\n",
    "    df = other_features(df)\n",
    "    gc.collect()  # Perform garbage collection to free up memory\n",
    "    \n",
    "    # Select and return the generated features\n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]\n",
    "\n",
    "def reduce_features(df):\n",
    "    cols = [d for d in df.columns.values if 'diff' not in d and 'shift' not in d and 'ret' not in d and d not in ['seconds', 'minute']]\n",
    "    return df[cols]\n",
    "\n",
    "def get_X(df: pd.DataFrame, drop_list=['target']) -> pd.DataFrame:\n",
    "    cols = [c for c in df.columns if c not in drop_list]\n",
    "    return df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea933a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:17:36.500711Z",
     "iopub.status.busy": "2023-11-17T04:17:36.500412Z",
     "iopub.status.idle": "2023-11-17T04:17:36.504204Z",
     "shell.execute_reply": "2023-11-17T04:17:36.503618Z",
     "shell.execute_reply.started": "2023-11-17T04:17:36.500694Z"
    },
    "papermill": {
     "duration": 0.017829,
     "end_time": "2023-11-08T11:55:33.415339",
     "exception": false,
     "start_time": "2023-11-08T11:55:33.397510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weights = [\n",
    "#     0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "#     0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "#     0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "#     0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "#     0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "#     0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "#     0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "#     0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "#     0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "#     0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "#     0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "#     0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "#     0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "#     0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "#     0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "#     0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "#     0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "# ]\n",
    "\n",
    "# weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963215c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:17:37.050911Z",
     "iopub.status.busy": "2023-11-17T04:17:37.050583Z",
     "iopub.status.idle": "2023-11-17T04:17:37.366809Z",
     "shell.execute_reply": "2023-11-17T04:17:37.366216Z",
     "shell.execute_reply.started": "2023-11-17T04:17:37.050890Z"
    },
    "papermill": {
     "duration": 0.478889,
     "end_time": "2023-11-08T11:55:33.902851",
     "exception": false,
     "start_time": "2023-11-08T11:55:33.423962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df.copy()\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc0a38f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:17:49.384166Z",
     "iopub.status.busy": "2023-11-17T04:17:49.383938Z",
     "iopub.status.idle": "2023-11-17T04:18:10.959232Z",
     "shell.execute_reply": "2023-11-17T04:18:10.958560Z",
     "shell.execute_reply.started": "2023-11-17T04:17:49.384154Z"
    },
    "papermill": {
     "duration": 62.215773,
     "end_time": "2023-11-08T11:56:36.127417",
     "exception": false,
     "start_time": "2023-11-08T11:55:33.911644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_stock_id_feats = {\n",
    "    \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "    \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "    \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "    \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "    \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "    \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "}\n",
    "\n",
    "X = generate_all_features(df_train)\n",
    "X = reduce_features(X)\n",
    "X = reduce_mem_usage(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c367472f-7263-4d33-bfbe-6954bbc02dc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:18:10.973503Z",
     "iopub.status.busy": "2023-11-17T04:18:10.973386Z",
     "iopub.status.idle": "2023-11-17T04:18:11.458753Z",
     "shell.execute_reply": "2023-11-17T04:18:11.458125Z",
     "shell.execute_reply.started": "2023-11-17T04:18:10.973493Z"
    }
   },
   "outputs": [],
   "source": [
    "X = get_X(X)\n",
    "y = df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c876b70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:18:11.459839Z",
     "iopub.status.busy": "2023-11-17T04:18:11.459723Z",
     "iopub.status.idle": "2023-11-17T04:18:11.464776Z",
     "shell.execute_reply": "2023-11-17T04:18:11.464256Z",
     "shell.execute_reply.started": "2023-11-17T04:18:11.459826Z"
    },
    "papermill": {
     "duration": 0.022876,
     "end_time": "2023-11-08T11:56:39.480823",
     "exception": false,
     "start_time": "2023-11-08T11:56:39.457947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237892, 62)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5237892,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['stock_id', 'seconds_in_bucket', 'imbalance_size',\n",
       "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
       "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
       "       'ask_size', 'wap', 'volume', 'mid_price', 'liquidity_imbalance',\n",
       "       'matched_imbalance', 'size_imbalance',\n",
       "       'reference_price_far_price_imb', 'reference_price_near_price_imb',\n",
       "       'reference_price_ask_price_imb', 'reference_price_bid_price_imb',\n",
       "       'reference_price_wap_imb', 'far_price_near_price_imb',\n",
       "       'far_price_ask_price_imb', 'far_price_bid_price_imb',\n",
       "       'far_price_wap_imb', 'near_price_ask_price_imb',\n",
       "       'near_price_bid_price_imb', 'near_price_wap_imb',\n",
       "       'ask_price_bid_price_imb', 'ask_price_wap_imb',\n",
       "       'bid_price_wap_imb', 'imbalance_momentum', 'price_spread',\n",
       "       'spread_intensity', 'price_pressure', 'market_urgency',\n",
       "       'depth_pressure', 'all_prices_mean', 'all_sizes_mean',\n",
       "       'all_prices_std', 'all_sizes_std', 'all_prices_skew',\n",
       "       'all_sizes_skew', 'all_prices_kurt', 'all_sizes_kurt',\n",
       "       'ask_price_bid_price_wap_imb2',\n",
       "       'ask_price_bid_price_reference_price_imb2',\n",
       "       'ask_price_wap_reference_price_imb2',\n",
       "       'bid_price_wap_reference_price_imb2',\n",
       "       'matched_size_bid_size_ask_size_imb2',\n",
       "       'matched_size_bid_size_imbalance_size_imb2',\n",
       "       'matched_size_ask_size_imbalance_size_imb2',\n",
       "       'bid_size_ask_size_imbalance_size_imb2', 'dow',\n",
       "       'global_median_size', 'global_std_size', 'global_ptp_size',\n",
       "       'global_median_price', 'global_std_price', 'global_ptp_price'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X.shape, y.shape)\n",
    "display(X.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b94a05c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:18:11.465379Z",
     "iopub.status.busy": "2023-11-17T04:18:11.465266Z",
     "iopub.status.idle": "2023-11-17T04:18:11.758328Z",
     "shell.execute_reply": "2023-11-17T04:18:11.757727Z",
     "shell.execute_reply.started": "2023-11-17T04:18:11.465369Z"
    },
    "papermill": {
     "duration": 0.441033,
     "end_time": "2023-11-08T11:56:39.931713",
     "exception": false,
     "start_time": "2023-11-08T11:56:39.490680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The total number of date_ids is 480, we split them into 5 folds with a gap of 5 days in between\n",
    "num_folds = 5\n",
    "fold_size = 480 // num_folds\n",
    "gap = 5\n",
    "folds = []\n",
    "\n",
    "# We need to use the date_id from df_train to split the data\n",
    "date_ids = df_train['date_id'].values\n",
    "\n",
    "for i in range(num_folds):\n",
    "    start = i * fold_size\n",
    "    end = start + fold_size\n",
    "    \n",
    "    # Define the training and testing sets by date_id\n",
    "    if i < num_folds - 1:  # No need to purge after the last fold\n",
    "        purged_start = end - 2\n",
    "        purged_end = end + gap + 2\n",
    "        train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n",
    "    else:\n",
    "        train_indices = (date_ids >= start) & (date_ids < end)\n",
    "    \n",
    "    test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n",
    "    folds.append((train_indices, test_indices))\n",
    "\n",
    "del date_ids, df_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7471cc5",
   "metadata": {
    "papermill": {
     "duration": 0.008782,
     "end_time": "2023-11-08T11:56:39.949872",
     "exception": false,
     "start_time": "2023-11-08T11:56:39.941090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf8383fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:23:14.883308Z",
     "iopub.status.busy": "2023-11-17T04:23:14.883138Z",
     "iopub.status.idle": "2023-11-17T04:23:14.928512Z",
     "shell.execute_reply": "2023-11-17T04:23:14.927885Z",
     "shell.execute_reply.started": "2023-11-17T04:23:14.883297Z"
    },
    "papermill": {
     "duration": 0.986508,
     "end_time": "2023-11-08T11:56:40.945202",
     "exception": false,
     "start_time": "2023-11-08T11:56:39.958694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "null_check_cols = []\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    \n",
    "def mae_metric(y_true, y_pred):\n",
    "    mae = np.mean(np.abs((y_true - y_pred)))\n",
    "    return mae\n",
    "\n",
    "# def rmspe_metric(y_true, y_pred):\n",
    "#     rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "#     return rmspe\n",
    "\n",
    "\n",
    "# def rmspe_loss(y_true, y_pred):\n",
    "#     rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n",
    "#     return rmspe\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n",
    "        super().__init__()\n",
    "        self.x_num = x_num\n",
    "        self.x_cat = x_cat\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n",
    "        else:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_num_dim: int,\n",
    "                 n_categories: List[int],\n",
    "                 dropout: float = 0.0,\n",
    "                 hidden: int = 50,\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 bn: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "        if bn:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x_all = torch.cat([x_num, x_cat_emb], 1)\n",
    "        x = self.sequence(x_all)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 hidden_size: int,\n",
    "                 n_categories: List[int],\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 channel_1: int = 256,\n",
    "                 channel_2: int = 512,\n",
    "                 channel_3: int = 512,\n",
    "                 dropout_top: float = 0.1,\n",
    "                 dropout_mid: float = 0.3,\n",
    "                 dropout_bottom: float = 0.2,\n",
    "                 weight_norm: bool = True,\n",
    "                 two_stage: bool = True,\n",
    "                 celu: bool = True,\n",
    "                 kernel1: int = 5,\n",
    "                 leaky_relu: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        num_targets = 1\n",
    "\n",
    "        cha_1_reshape = int(hidden_size / channel_1)\n",
    "        cha_po_1 = int(hidden_size / channel_1 / 2)\n",
    "        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n",
    "\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.cha_1 = channel_1\n",
    "        self.cha_2 = channel_2\n",
    "        self.cha_3 = channel_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features + self.cat_dim),\n",
    "            nn.Dropout(dropout_top),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n",
    "            nn.CELU(0.06) if celu else nn.ReLU()\n",
    "        )\n",
    "\n",
    "        def _norm(layer, dim=None):\n",
    "            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_1),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n",
    "            nn.BatchNorm1d(channel_2),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if self.two_stage:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_mid),\n",
    "                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        if leaky_relu:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n",
    "            )\n",
    "\n",
    "        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x = torch.cat([x_num, x_cat_emb], 1)\n",
    "\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.two_stage:\n",
    "            x = self.conv2(x) * x\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "        x = self.flt(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "def preprocess_nn(\n",
    "        X: pd.DataFrame,\n",
    "        scaler: Optional[StandardScaler] = None,\n",
    "        scaler_type: str = 'standard',\n",
    "        n_pca: int = -1,\n",
    "        na_cols: bool = True):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    if na_cols:\n",
    "        #for c in X.columns:\n",
    "        for c in null_check_cols:\n",
    "            if c in X.columns:\n",
    "                X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n",
    "\n",
    "    cat_cols = [c for c in X.columns if c in ['stock_id']]\n",
    "#     cat_cols = [c for c in X.columns if c in ['date_id', 'stock_id']]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    X_num = X[num_cols].values.astype(np.float32)\n",
    "    X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n",
    "\n",
    "    def _pca(X_num_):\n",
    "        if n_pca > 0:\n",
    "            pca = PCA(n_components=n_pca, random_state=0)\n",
    "            return pca.fit_transform(X_num)\n",
    "        return X_num\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols, scaler\n",
    "    else:\n",
    "        X_num = scaler.transform(X_num) #TODO: infでも大丈夫？\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols\n",
    "\n",
    "\n",
    "def train_epoch(data_loader: DataLoader,\n",
    "                model: nn.Module,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                device,\n",
    "                clip_grad: float = 1.5):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    step = 0\n",
    "\n",
    "    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n",
    "        batch_size = x_num.size(0)\n",
    "        x_num = x_num.to(device, dtype=torch.float)\n",
    "        x_cat = x_cat.to(device)\n",
    "        y = y.to(device, dtype=torch.float)\n",
    "        \n",
    "        output = model(x_num, x_cat)\n",
    "        criterion = nn.L1Loss()\n",
    "        loss = criterion(y, output)\n",
    "\n",
    "        losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def evaluate(data_loader: DataLoader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            batch_size = x_num.size(0)\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "            y = y.to(device, dtype=torch.float)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(x_num, x_cat)\n",
    "\n",
    "            criterion = nn.L1Loss()\n",
    "            loss = criterion(y, output)\n",
    "\n",
    "            # record loss\n",
    "            losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "\n",
    "            targets = y.detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "\n",
    "            final_targets.append(targets)\n",
    "            final_outputs.append(output)\n",
    "\n",
    "    final_targets = np.concatenate(final_targets)\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "\n",
    "    try:\n",
    "        metric = mae_metric(final_targets, final_outputs)\n",
    "    except:\n",
    "        metric = None\n",
    "\n",
    "    return final_outputs, final_targets, losses.avg, metric\n",
    "\n",
    "\n",
    "def predict_nn(X: pd.DataFrame,\n",
    "               model: Union[List[MLP], MLP],\n",
    "               scaler: StandardScaler,\n",
    "               device,\n",
    "               ensemble_method='mean'):\n",
    "    '''\n",
    "    '''\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    for m in model:\n",
    "        m.eval()\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    valid_dataset = TabularDataset(X_num, X_cat, None)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=512,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=CFG.num_workers)\n",
    "\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "\n",
    "            outputs = []\n",
    "            with torch.no_grad():\n",
    "                for m in model:\n",
    "                    output = m(x_num, x_cat)\n",
    "                    outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "            if ensemble_method == 'median':\n",
    "                pred = np.nanmedian(np.array(outputs), axis=0)\n",
    "            else:\n",
    "                pred = np.array(outputs).mean(axis=0)\n",
    "            final_outputs.append(pred)\n",
    "\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "    return final_outputs\n",
    "\n",
    "\n",
    "def train_nn(X: pd.DataFrame,\n",
    "             y: pd.DataFrame,\n",
    "             folds: List[Tuple],\n",
    "             device,\n",
    "             emb_dim: int = 25,\n",
    "             batch_size: int = 1024,\n",
    "             model_type: str = 'mlp',\n",
    "             mlp_dropout: float = 0.0,\n",
    "             mlp_hidden: int = 64,\n",
    "             mlp_bn: bool = False,\n",
    "             cnn_hidden: int = 64,\n",
    "             cnn_channel1: int = 32,\n",
    "             cnn_channel2: int = 32,\n",
    "             cnn_channel3: int = 32,\n",
    "             cnn_kernel1: int = 5,\n",
    "             cnn_celu: bool = False,\n",
    "             cnn_weight_norm: bool = False,\n",
    "             dropout_emb: bool = 0.0,\n",
    "             lr: float = 1e-3,\n",
    "             weight_decay: float = 0.0,\n",
    "             model_path: str = 'fold_{}.pth',\n",
    "             scaler_type: str = 'standard',\n",
    "             output_dir: str = 'artifacts',\n",
    "             scheduler_type: str = 'onecycle',\n",
    "             optimizer_type: str = 'adam',\n",
    "             max_lr: float = 0.01,\n",
    "             epochs: int = 30,\n",
    "             seed: int = 42,\n",
    "             n_pca: int = -1,\n",
    "             batch_double_freq: int = 50,\n",
    "             cnn_dropout: float = 0.1,\n",
    "             na_cols: bool = True,\n",
    "             cnn_leaky_relu: bool = False,\n",
    "             patience: int = 8,\n",
    "             factor: float = 0.5):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "\n",
    "        cur_batch = batch_size\n",
    "        best_loss = 1e10\n",
    "        best_prediction = None\n",
    "\n",
    "        print(f\"fold {cv_idx} train: {X_tr.shape}, valid: {X_va.shape}\")\n",
    "\n",
    "        train_dataset = TabularDataset(X_tr, X_tr_cat, y_tr)\n",
    "        valid_dataset = TabularDataset(X_va, X_va_cat, y_va)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cur_batch, shuffle=True,\n",
    "                                                   num_workers=CFG.num_workers)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cur_batch, shuffle=False,\n",
    "                                                   num_workers=CFG.num_workers)\n",
    "\n",
    "        if model_type == 'mlp':\n",
    "            model = MLP(X_tr.shape[1],\n",
    "                        n_categories=[256],\n",
    "                        dropout=mlp_dropout, hidden=mlp_hidden, emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb, bn=mlp_bn)\n",
    "        elif model_type == 'cnn':\n",
    "            model = CNN(X_tr.shape[1],\n",
    "                        hidden_size=cnn_hidden,\n",
    "                        n_categories=[128],\n",
    "                        emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb,\n",
    "                        channel_1=cnn_channel1,\n",
    "                        channel_2=cnn_channel2,\n",
    "                        channel_3=cnn_channel3,\n",
    "                        two_stage=False,\n",
    "                        kernel1=cnn_kernel1,\n",
    "                        celu=cnn_celu,\n",
    "                        dropout_top=cnn_dropout,\n",
    "                        dropout_mid=cnn_dropout,\n",
    "                        dropout_bottom=cnn_dropout,\n",
    "                        weight_norm=cnn_weight_norm,\n",
    "                        leaky_relu=cnn_leaky_relu)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        model = model.to(device)\n",
    "\n",
    "        if optimizer_type == 'adamw':\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'adam':\n",
    "            opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        scheduler = epoch_scheduler = None\n",
    "        if scheduler_type == 'onecycle':\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n",
    "                                                            max_lr=max_lr, epochs=epochs,\n",
    "                                                            steps_per_epoch=len(train_loader))\n",
    "        elif scheduler_type == 'reduce':\n",
    "            epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n",
    "                                                                         mode='min',\n",
    "                                                                         min_lr=1e-7,\n",
    "                                                                         patience=patience,\n",
    "                                                                         verbose=True,\n",
    "                                                                         factor=factor)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if epoch > 0 and epoch % batch_double_freq == 0:\n",
    "                cur_batch = cur_batch * 2\n",
    "                print(f'batch: {cur_batch}')\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                           batch_size=cur_batch,\n",
    "                                                           shuffle=True,\n",
    "                                                           num_workers=CFG.num_workers)\n",
    "            train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n",
    "            predictions, valid_targets, valid_loss, valid_mae = evaluate(valid_loader, model, device=device)\n",
    "            print(f\"epoch {epoch}, train loss: {train_loss:.3f}, valid loss: {valid_loss:.3f}, valid mae: {valid_mae:.3f}\")\n",
    "\n",
    "            if epoch_scheduler is not None:\n",
    "                epoch_scheduler.step(valid_mae)\n",
    "\n",
    "            if valid_mae < best_loss:\n",
    "                print(f'new best:{valid_mae}')\n",
    "                best_loss = valid_mae\n",
    "                best_prediction = predictions\n",
    "                torch.save(model, os.path.join(output_dir, model_path.format(cv_idx)))\n",
    "\n",
    "        best_predictions.append(best_prediction)\n",
    "        best_losses.append(best_loss)\n",
    "        del model, train_dataset, valid_dataset, train_loader, valid_loader, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, opt\n",
    "        if scheduler is not None:\n",
    "            del scheduler\n",
    "        gc.collect()\n",
    "\n",
    "    return best_losses, best_predictions, scaler\n",
    "\n",
    "def get_top_n_models(models, scores, top_n):\n",
    "    if len(models) <= top_n:\n",
    "        print('number of models are less than top_n. all models will be used')\n",
    "        return models\n",
    "    sorted_ = [(y, x) for y, x in sorted(zip(scores, models), key=lambda pair: pair[0])]\n",
    "    print(f'scores(sorted): {[y for y, _ in sorted_]}')\n",
    "    return [x for _, x in sorted_][:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8738b05d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T05:57:56.173338Z",
     "iopub.status.busy": "2023-11-14T05:57:56.173106Z",
     "iopub.status.idle": "2023-11-14T05:57:56.175455Z",
     "shell.execute_reply": "2023-11-14T05:57:56.175100Z",
     "shell.execute_reply.started": "2023-11-14T05:57:56.173326Z"
    },
    "papermill": {
     "duration": 0.016508,
     "end_time": "2023-11-08T11:56:40.971091",
     "exception": false,
     "start_time": "2023-11-08T11:56:40.954583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# folds = []\n",
    "# gkf = GroupKFold(n_splits=4)\n",
    "\n",
    "# for i, (idx_train, idx_valid) in enumerate(gkf.split(X, None, groups=train['stock_id'])):\n",
    "#     folds.append((idx_train, idx_valid))\n",
    "# folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff62a98",
   "metadata": {
    "papermill": {
     "duration": 0.009007,
     "end_time": "2023-11-08T11:56:40.989122",
     "exception": false,
     "start_time": "2023-11-08T11:56:40.980115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb5cfa8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T05:57:56.951980Z",
     "iopub.status.busy": "2023-11-14T05:57:56.951440Z",
     "iopub.status.idle": "2023-11-14T05:57:56.959378Z",
     "shell.execute_reply": "2023-11-14T05:57:56.958723Z",
     "shell.execute_reply.started": "2023-11-14T05:57:56.951958Z"
    },
    "papermill": {
     "duration": 4429.84814,
     "end_time": "2023-11-08T13:10:30.846271",
     "exception": false,
     "start_time": "2023-11-08T11:56:40.998131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.TRAINING and 'MLP' in CFG.methods:\n",
    "    model_save_path = f'{CFG.model_path}/mlp' \n",
    "    model_paths = []\n",
    "    scores = []\n",
    "    \n",
    "    if CFG.is_test_mode:\n",
    "        print('RUNNING MLP TEST MODE...')\n",
    "        epochs = 3\n",
    "        try_num = 1\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 30\n",
    "        try_num = 10\n",
    "        valid_th = 5\n",
    "    \n",
    "    for i in tqdm(range(try_num)):\n",
    "        nn_losses, nn_preds, scaler = train_nn(X, y, \n",
    "                                               folds, \n",
    "                                               device=CFG.device, \n",
    "                                               batch_size=512,\n",
    "                                               mlp_bn=True,\n",
    "                                               mlp_hidden=256,\n",
    "                                               mlp_dropout=0.0,\n",
    "                                               emb_dim=30,\n",
    "                                               epochs=epochs,\n",
    "                                               lr=0.002,\n",
    "                                               max_lr=0.0055,\n",
    "                                               weight_decay=1e-7,\n",
    "                                               output_dir=model_save_path,\n",
    "                                               model_path='mlp_fold_{}' + f\"_seed{i}.pth\",\n",
    "                                               seed=i)\n",
    "        loss_min_idx = np.argmin(nn_losses)\n",
    "        if nn_losses[loss_min_idx] < valid_th:\n",
    "            print(f'model of seed {i} added.')\n",
    "            scores.append(nn_losses[loss_min_idx])\n",
    "            model_paths.append(f'{model_save_path}/mlp_fold_{loss_min_idx}_seed{i}.pth')\n",
    "            np.save(f'pred_mlp_seed{i}.npy', nn_preds[loss_min_idx])\n",
    "    \n",
    "    model_paths = get_top_n_models(model_paths, scores, 3)\n",
    "    mlp_model = [torch.load(path, CFG.device) for path in model_paths]\n",
    "    print(f'total {len(mlp_model)} models will be used.')\n",
    "    print(f'better models: {model_paths}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cdf5fc",
   "metadata": {
    "papermill": {
     "duration": 3.157238,
     "end_time": "2023-11-08T13:10:37.282023",
     "exception": false,
     "start_time": "2023-11-08T13:10:34.124785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad42d240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T05:57:58.096536Z",
     "iopub.status.busy": "2023-11-14T05:57:58.096186Z",
     "iopub.status.idle": "2023-11-14T05:57:58.105037Z",
     "shell.execute_reply": "2023-11-14T05:57:58.104355Z",
     "shell.execute_reply.started": "2023-11-14T05:57:58.096512Z"
    },
    "papermill": {
     "duration": 3.247441,
     "end_time": "2023-11-08T13:10:43.680514",
     "exception": false,
     "start_time": "2023-11-08T13:10:40.433073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.TRAINING and 'CNN' in CFG.methods:\n",
    "    model_save_path = f'{CFG.model_path}/cnn'\n",
    "    model_paths = []\n",
    "    scores = []\n",
    "    \n",
    "    if CFG.is_test_mode:\n",
    "        print('RUNNING CNN TEST MODE...')\n",
    "        epochs = 3\n",
    "        try_num = 1\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 50\n",
    "        try_num = 10\n",
    "        valid_th = 5\n",
    "    \n",
    "    for i in tqdm(range(try_num)):\n",
    "        nn_losses, nn_preds, scaler = train_nn(X, y, \n",
    "                                               folds, \n",
    "                                               device=CFG.device, \n",
    "                                               cnn_hidden=8*128,\n",
    "                                               batch_size=1280,\n",
    "                                               model_type='cnn',\n",
    "                                               emb_dim=30,\n",
    "                                               epochs=epochs,\n",
    "                                               cnn_channel1=128,\n",
    "                                               cnn_channel2=3*128,\n",
    "                                               cnn_channel3=3*128,\n",
    "                                               lr=0.00038, #0.0011,\n",
    "                                               max_lr=0.0013,\n",
    "                                               weight_decay=6.5e-6,\n",
    "                                               optimizer_type='adam',\n",
    "                                               scheduler_type='reduce',\n",
    "                                               output_dir=model_save_path,\n",
    "                                               model_path='cnn_fold_{}' + f'_seed{i}.pth',\n",
    "                                               seed=i,\n",
    "                                               cnn_dropout=0.0,\n",
    "                                               cnn_weight_norm=True,\n",
    "                                               cnn_leaky_relu=False,\n",
    "                                               patience=8,\n",
    "                                               factor=0.3)\n",
    "        loss_min_idx = np.argmin(nn_losses)\n",
    "        if nn_losses[loss_min_idx] < valid_th:\n",
    "            print(f'model of seed {i} added.')\n",
    "            scores.append(nn_losses[loss_min_idx])\n",
    "            model_paths.append(f'{model_save_path}/cnn_fold_{loss_min_idx}_seed{i}.pth')\n",
    "            np.save(f'pred_cnn_seed{i}.npy', nn_preds[loss_min_idx])\n",
    "            \n",
    "    model_paths = get_top_n_models(model_paths, scores, 3)\n",
    "    cnn_model = [torch.load(path, CFG.device) for path in model_paths]\n",
    "    print(f'total {len(cnn_model)} models will be used.')\n",
    "    print(f'better models: {model_paths}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89207b08",
   "metadata": {
    "papermill": {
     "duration": 3.154806,
     "end_time": "2023-11-08T13:10:50.255782",
     "exception": false,
     "start_time": "2023-11-08T13:10:47.100976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d94648f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:19:39.963978Z",
     "iopub.status.busy": "2023-11-17T04:19:39.963806Z",
     "iopub.status.idle": "2023-11-17T04:19:45.382690Z",
     "shell.execute_reply": "2023-11-17T04:19:45.381732Z",
     "shell.execute_reply.started": "2023-11-17T04:19:39.963967Z"
    },
    "papermill": {
     "duration": 35.852011,
     "end_time": "2023-11-08T13:11:29.385586",
     "exception": false,
     "start_time": "2023-11-08T13:10:53.533575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4107ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T04:23:19.679529Z",
     "iopub.status.busy": "2023-11-17T04:23:19.679174Z"
    },
    "papermill": {
     "duration": 3.286446,
     "end_time": "2023-11-08T13:11:35.842065",
     "exception": false,
     "start_time": "2023-11-08T13:11:32.555619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1098902825546732.6| val_0_unsup_loss_numpy: 203179.28125|  0:00:14s\n",
      "epoch 10 | loss: 38986609.82363| val_0_unsup_loss_numpy: 1.5304700136184692|  0:02:44s\n",
      "epoch 20 | loss: 9745668706.58564| val_0_unsup_loss_numpy: 1.177590012550354|  0:05:12s\n",
      "epoch 30 | loss: 435128342779.8608| val_0_unsup_loss_numpy: 1.254580020904541|  0:07:42s\n",
      "epoch 40 | loss: 174228963.08498| val_0_unsup_loss_numpy: 1.5782300233840942|  0:10:10s\n",
      "epoch 50 | loss: 357905.55558| val_0_unsup_loss_numpy: 1.0044000148773193|  0:12:36s\n",
      "epoch 60 | loss: 173821.17095| val_0_unsup_loss_numpy: 0.9906100034713745|  0:15:04s\n",
      "epoch 70 | loss: 210740170888.813| val_0_unsup_loss_numpy: 0.9905400276184082|  0:17:34s\n",
      "epoch 80 | loss: 206642.68168| val_0_unsup_loss_numpy: 0.9891899824142456|  0:20:04s\n",
      "epoch 90 | loss: 81539.30191| val_0_unsup_loss_numpy: 0.9951099753379822|  0:22:32s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 51 and best_val_0_unsup_loss_numpy = 0.9860900044441223\n",
      "epoch 0  | loss: 6.37399 | val_0_mae: 7.111110210418701|  0:00:15s\n",
      "epoch 10 | loss: 6.25973 | val_0_mae: 7.048329830169678|  0:02:47s\n",
      "epoch 20 | loss: 6.23464 | val_0_mae: 7.0184102058410645|  0:05:19s\n",
      "epoch 30 | loss: 6.21724 | val_0_mae: 7.001840114593506|  0:07:51s\n",
      "epoch 40 | loss: 6.2014  | val_0_mae: 7.0023698806762695|  0:10:24s\n",
      "epoch 50 | loss: 6.18888 | val_0_mae: 6.964449882507324|  0:12:57s\n",
      "epoch 60 | loss: 6.17869 | val_0_mae: 6.958960056304932|  0:15:29s\n",
      "epoch 70 | loss: 6.16868 | val_0_mae: 6.953030109405518|  0:18:02s\n",
      "epoch 80 | loss: 6.16117 | val_0_mae: 6.97475004196167|  0:20:35s\n",
      "epoch 90 | loss: 6.15459 | val_0_mae: 6.932370185852051|  0:23:07s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 94 and best_val_0_mae = 6.927720069885254\n",
      "Successfully saved model at /kaggle/input/optiver-trading-at-the-close/tabnet/tabnet_fold_0_seed0.pth.zip\n",
      "epoch 0  | loss: 829303196282282.8| val_0_unsup_loss_numpy: 346193.25|  0:00:12s\n",
      "epoch 10 | loss: 4276994.76176| val_0_unsup_loss_numpy: 31.604639053344727|  0:02:21s\n",
      "epoch 20 | loss: 247746.63945| val_0_unsup_loss_numpy: 1098.5146484375|  0:04:27s\n",
      "epoch 30 | loss: 631449.42081| val_0_unsup_loss_numpy: 1.1077100038528442|  0:06:33s\n",
      "epoch 40 | loss: 795989.03816| val_0_unsup_loss_numpy: 1.0705699920654297|  0:08:39s\n",
      "epoch 50 | loss: 358696.14898| val_0_unsup_loss_numpy: 1.4829299449920654|  0:10:45s\n",
      "epoch 60 | loss: 757980.48112| val_0_unsup_loss_numpy: 1.2502100467681885|  0:12:50s\n",
      "epoch 70 | loss: 264268.18209| val_0_unsup_loss_numpy: 1.398069977760315|  0:14:58s\n",
      "epoch 80 | loss: 318938.7875| val_0_unsup_loss_numpy: 1.6994999647140503|  0:17:07s\n",
      "\n",
      "Early stopping occurred at epoch 82 with best_epoch = 32 and best_val_0_unsup_loss_numpy = 0.9868599772453308\n",
      "epoch 0  | loss: 6.45877 | val_0_mae: 6.4562602043151855|  0:00:12s\n",
      "epoch 10 | loss: 6.36827 | val_0_mae: 6.3913397789001465|  0:02:23s\n",
      "epoch 20 | loss: 6.34472 | val_0_mae: 6.3685197830200195|  0:04:33s\n",
      "epoch 30 | loss: 6.32426 | val_0_mae: 6.344639778137207|  0:06:43s\n",
      "epoch 40 | loss: 6.3068  | val_0_mae: 6.333749771118164|  0:08:53s\n",
      "epoch 50 | loss: 6.29243 | val_0_mae: 6.318819999694824|  0:11:04s\n",
      "epoch 60 | loss: 6.27808 | val_0_mae: 6.304100036621094|  0:13:22s\n",
      "epoch 70 | loss: 6.26777 | val_0_mae: 6.299570083618164|  0:15:34s\n",
      "epoch 80 | loss: 6.25587 | val_0_mae: 6.287909984588623|  0:17:50s\n",
      "epoch 90 | loss: 6.24694 | val_0_mae: 6.279930114746094|  0:20:02s\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.metrics import MAE\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "\n",
    "def MAELoss_Tabnet(y_pred, y_true):\n",
    "    return torch.mean(torch.abs((y_true - y_pred))).clone()\n",
    "\n",
    "\n",
    "def predict_tabnet(X: pd.DataFrame,\n",
    "                   model: Union[List[TabNetRegressor], TabNetRegressor],\n",
    "                   scaler: StandardScaler,\n",
    "                   ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    X_processed = np.concatenate([X_cat, X_num], axis=1)\n",
    "\n",
    "    predicted = []\n",
    "    for m in model:\n",
    "        predicted.append(m.predict(X_processed))\n",
    "\n",
    "    if ensemble_method == 'median':\n",
    "        pred = np.nanmedian(np.array(predicted), axis=0)\n",
    "    else:\n",
    "        pred = np.array(predicted).mean(axis=0)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def train_tabnet(X: pd.DataFrame,\n",
    "                 y: pd.DataFrame,\n",
    "                 folds: List[Tuple],\n",
    "                 batch_size: int = 1024,\n",
    "                 lr: float = 1e-3,\n",
    "                 model_path: str = 'fold_{}.pth',\n",
    "                 scaler_type: str = 'standard',\n",
    "                 output_dir: str = 'artifacts',\n",
    "                 epochs: int = 250,\n",
    "                 seed: int = 42,\n",
    "                 n_pca: int = -1,\n",
    "                 na_cols: bool = True,\n",
    "                 patience: int = 10,\n",
    "                 factor: float = 0.5,\n",
    "                 gamma: float = 2.0,\n",
    "                 lambda_sparse: float = 8.0,\n",
    "                 n_steps: int = 2,\n",
    "                 scheduler_type: str = 'cosine',\n",
    "                 n_a: int = 16):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "        y_tr = y_tr.reshape(-1,1)\n",
    "        y_va = y_va.reshape(-1,1)\n",
    "        X_tr = np.concatenate([X_tr_cat, X_tr], axis=1)\n",
    "        X_va = np.concatenate([X_va_cat, X_va], axis=1)\n",
    "\n",
    "        cat_idxs = list(range(X_cat.shape[1]))\n",
    "        cat_dims = [np.unique(X_cat[:, i]).size for i in cat_idxs]\n",
    "        \n",
    "        if scheduler_type == 'cosine':\n",
    "            scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False)\n",
    "            scheduler_fn = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "        else:\n",
    "            scheduler_params = {'mode': 'min', 'min_lr': 1e-7, 'patience': patience, 'factor': factor, 'verbose': True}\n",
    "            scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "\n",
    "        tabnet_params = dict(\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=1,\n",
    "            n_d=n_a,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            n_independent=2,\n",
    "            n_shared=2,\n",
    "            lambda_sparse=lambda_sparse,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params={'lr': lr},\n",
    "            mask_type=\"entmax\",\n",
    "            scheduler_fn=scheduler_fn,\n",
    "            scheduler_params=scheduler_params,\n",
    "            seed=seed,\n",
    "            verbose=10,\n",
    "            device_name='auto' if torch.cuda.is_available() and CFG.is_gpu else 'cpu',\n",
    "        )\n",
    "\n",
    "        pretrainer = TabNetPretrainer(**tabnet_params)\n",
    "        pretrainer.fit(\n",
    "            X_tr, eval_set=[X_va], max_epochs=epochs, patience=50, batch_size=1024*20,\n",
    "            virtual_batch_size=batch_size, num_workers=CFG.num_workers, drop_last=False)\n",
    "\n",
    "        model = TabNetRegressor(**tabnet_params)\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], max_epochs=epochs, patience=50, batch_size=1024*20,\n",
    "                  virtual_batch_size=batch_size, num_workers=CFG.num_workers, drop_last=False, eval_metric=[MAE], loss_fn=MAELoss_Tabnet,\n",
    "                  from_unsupervised=pretrainer)\n",
    "\n",
    "        path = os.path.join(output_dir, model_path.format(cv_idx))\n",
    "        model.save_model(path)\n",
    "\n",
    "        predicted = model.predict(X_va)\n",
    "\n",
    "        mae = mae_metric(y_va, predicted)\n",
    "        best_losses.append(mae)\n",
    "        best_predictions.append(predicted)\n",
    "\n",
    "\n",
    "    del pretrainer, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, scheduler_fn\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return best_losses, best_predictions, scaler, model\n",
    "\n",
    "\n",
    "if CFG.TRAINING and 'TABNET' in CFG.methods:\n",
    "    model_save_path = f'{CFG.model_path}tabnet'\n",
    "    model_paths = []\n",
    "    tab_model = []\n",
    "    scores = []\n",
    "\n",
    "    if CFG.is_test_mode:\n",
    "        print('RUNNING TABNET TEST MODE...')\n",
    "        epochs = 3\n",
    "        try_num = 1\n",
    "        valid_th = 1000\n",
    "    else:\n",
    "        epochs = 100\n",
    "        try_num = 5\n",
    "        valid_th = 5\n",
    "\n",
    "    for i in range(try_num):\n",
    "        nn_losses, nn_preds, scaler, model = train_tabnet(X, y,  \n",
    "                                                          folds, \n",
    "                                                          batch_size=1280,\n",
    "                                                          epochs=epochs,\n",
    "                                                          lr=0.04,\n",
    "                                                          patience=50,\n",
    "                                                          factor=0.5,\n",
    "                                                          gamma=1.6,\n",
    "                                                          lambda_sparse=3.55e-6,\n",
    "                                                          output_dir=model_save_path,\n",
    "                                                          model_path='tabnet_fold_{}' + f'_seed{i}.pth',\n",
    "                                                          seed=i,\n",
    "                                                          n_a=36)\n",
    "        loss_min_idx = np.argmin(nn_losses)\n",
    "        if nn_losses[loss_min_idx] < valid_th:\n",
    "            print(f'model of seed {i} added.')\n",
    "            tab_model.append(model)\n",
    "            scores.append(nn_losses[loss_min_idx])\n",
    "            model_paths.append(f'{model_save_path}/tabnet_fold_{loss_min_idx}_seed{i}.pth')\n",
    "            np.save(f'pred_tabnet_seed{i}.npy', nn_preds[loss_min_idx])\n",
    "\n",
    "    tab_model = get_top_n_models(tab_model, scores, 3)\n",
    "    print(f'total {len(tab_model)} models will be used.')\n",
    "    print(f'better models: {model_paths}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb65c04-311d-4877-8436-e912e0bfc266",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacee3d7-133c-4130-a0e6-7f16914afcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model.tabnet.feature_importances_\n",
    "feature_names = X.drop(\"target\", axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97faf4a-f3bc-4ae8-8830-ef3be3232159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重要度と特徴量の名前を辞書に格納\n",
    "importance_dict = dict(zip(feature_names, importance))\n",
    "importance_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704f5c0d-c11b-4e01-93e7-50d1c63b5039",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3f64b39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T13:11:48.787371Z",
     "iopub.status.busy": "2023-11-08T13:11:48.787010Z",
     "iopub.status.idle": "2023-11-08T13:11:48.798928Z",
     "shell.execute_reply": "2023-11-08T13:11:48.798102Z"
    },
    "papermill": {
     "duration": 3.295298,
     "end_time": "2023-11-08T13:11:48.800765",
     "exception": false,
     "start_time": "2023-11-08T13:11:45.505467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out\n",
    "\n",
    "if CFG.INFERENCE:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    counter = 0\n",
    "    y_min, y_max = -64, 64\n",
    "    qps, predictions = [], []\n",
    "    cache = pd.DataFrame()\n",
    "    \n",
    "    mlp_models = [torch.load(path, CFG.device) for path in model_paths]\n",
    "    # Weights for each fold model\n",
    "#     mlp_model_weights = [1/len(mlp_models)] * len(mlp_models)\n",
    "\n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        now_time = time.time()\n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "        feat = generate_all_features(cache)[-len(test):]\n",
    "        X = get_X(feat)\n",
    "\n",
    "        X_mlp = reduce_features(X)\n",
    "        # Generate predictions for each model and calculate the weighted average\n",
    "#         mlp_predictions = np.zeros(len(test))\n",
    "        mlp_predictions = predict_nn(X_mlp, mlp_models, scaler, device=CFG.device)\n",
    "#         for model, weight in zip(mlp_models, model_weights):\n",
    "#             mlp_predictions += weight * predict_nn(X)\n",
    "\n",
    "        mlp_predictions = zero_sum(mlp_predictions, test['bid_size']+test['ask_size'])\n",
    "        clipped_predictions = np.clip(mlp_predictions, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e0904",
   "metadata": {
    "papermill": {
     "duration": 3.319223,
     "end_time": "2023-11-08T13:11:55.321068",
     "exception": false,
     "start_time": "2023-11-08T13:11:52.001845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/discussion/274970\n",
    "- https://www.kaggle.com/code/nyanpn/1st-place-public-2nd-place-solution/notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4614.63454,
   "end_time": "2023-11-08T13:12:01.473404",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-08T11:55:06.838864",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
