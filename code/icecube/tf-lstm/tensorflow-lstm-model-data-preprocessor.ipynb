{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e5a95a",
   "metadata": {
    "papermill": {
     "duration": 0.003139,
     "end_time": "2023-03-15T22:17:46.978730",
     "exception": false,
     "start_time": "2023-03-15T22:17:46.975591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The code in this notebook is based on the code as was originally written in [LSTM Preprocessing Point Picker](https://www.kaggle.com/code/seungmoklee/lstm-preprocessing-point-picker). The author of this notebook did a great job of setting a clear baseline.\n",
    "\n",
    "I modified the code in the following part:\n",
    "* Maximum pulse count is set to 96.\n",
    "* Remove the features r_err and z_err.\n",
    "* Remove all non-essential code and graphics. \n",
    "\n",
    "With these few changes the output files only contain the features for the events as I use them in my [Tensorflow LSTM Model Training TPU](https://www.kaggle.com/code/rsmits/tensorflow-lstm-model-training-tpu) notebook and [Tensorflow LSTM Model Inference](https://www.kaggle.com/code/rsmits/tensorflow-lstm-model-inference) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6614f085",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T22:17:46.984913Z",
     "iopub.status.busy": "2023-03-15T22:17:46.984543Z",
     "iopub.status.idle": "2023-03-15T22:17:47.080751Z",
     "shell.execute_reply": "2023-03-15T22:17:47.079992Z"
    },
    "papermill": {
     "duration": 0.102033,
     "end_time": "2023-03-15T22:17:47.083210",
     "exception": false,
     "start_time": "2023-03-15T22:17:46.981177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data I/O and preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# System\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# multiprocessing\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb4e94e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T22:17:47.089556Z",
     "iopub.status.busy": "2023-03-15T22:17:47.088720Z",
     "iopub.status.idle": "2023-03-15T22:17:47.093974Z",
     "shell.execute_reply": "2023-03-15T22:17:47.093301Z"
    },
    "papermill": {
     "duration": 0.010361,
     "end_time": "2023-03-15T22:17:47.095931",
     "exception": false,
     "start_time": "2023-03-15T22:17:47.085570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data setting\n",
    "train_batch_id_first = 100\n",
    "train_batch_id_last = 200\n",
    "train_batch_ids = range(train_batch_id_first, train_batch_id_last + 1)\n",
    "\n",
    "# Feature Settings\n",
    "max_pulse_count = 96\n",
    "n_features = 7  # time, charge, aux, x, y, z, rank \n",
    "\n",
    "# Directories\n",
    "home_dir = \"/kaggle/input/icecube-neutrinos-in-deep-ice/\"\n",
    "train_format = home_dir + 'train/batch_{batch_id:d}.parquet'\n",
    "point_picker_format = '/kaggle/input/icecubedata/pp_mpc96_n7_batch_{batch_id:d}.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8861b129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T22:17:47.103042Z",
     "iopub.status.busy": "2023-03-15T22:17:47.102327Z",
     "iopub.status.idle": "2023-03-15T22:17:47.134404Z",
     "shell.execute_reply": "2023-03-15T22:17:47.133416Z"
    },
    "papermill": {
     "duration": 0.038889,
     "end_time": "2023-03-15T22:17:47.137100",
     "exception": false,
     "start_time": "2023-03-15T22:17:47.098211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time valid length: 6199.700247193777 ns\n"
     ]
    }
   ],
   "source": [
    "# Sensor Geometry Data\n",
    "sensor_geometry_df = pd.read_csv(home_dir + \"sensor_geometry.csv\")\n",
    "\n",
    "# X, Y, Z coordinates\n",
    "sensor_x = sensor_geometry_df.x\n",
    "sensor_y = sensor_geometry_df.y\n",
    "sensor_z = sensor_geometry_df.z\n",
    "\n",
    "# Detector constants\n",
    "c_const = 0.299792458  # speed of light [m/ns]\n",
    "\n",
    "# Min / Max information\n",
    "x_min = sensor_x.min()\n",
    "x_max = sensor_x.max()\n",
    "y_min = sensor_y.min()\n",
    "y_max = sensor_y.max()\n",
    "z_min = sensor_z.min()\n",
    "z_max = sensor_z.max()\n",
    "\n",
    "# Detector Valid Length\n",
    "detector_length = np.sqrt((x_max - x_min)**2 + (y_max - y_min)**2 + (z_max - z_min)**2)\n",
    "t_valid_length = detector_length / c_const\n",
    "\n",
    "print(f\"time valid length: {t_valid_length} ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78dc6e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T22:17:47.144298Z",
     "iopub.status.busy": "2023-03-15T22:17:47.143962Z",
     "iopub.status.idle": "2023-03-15T22:17:47.157248Z",
     "shell.execute_reply": "2023-03-15T22:17:47.156413Z"
    },
    "papermill": {
     "duration": 0.019405,
     "end_time": "2023-03-15T22:17:47.159645",
     "exception": false,
     "start_time": "2023-03-15T22:17:47.140240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "## Single event reader function\n",
    "\n",
    "- Pick-up important data points first\n",
    "    - Rank 3 (First)\n",
    "        - not aux, in valid time window\n",
    "    - Rank 2\n",
    "        - not aux, out of valid time window\n",
    "    - Rank 1\n",
    "        - aux, in valid time window\n",
    "    - Rank 0 (Last)\n",
    "        - aux, out of valid time window\n",
    "    - In each ranks, take pulses from highest charge\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# read single event from batch_meta_df\n",
    "def read_event(event_idx, batch_meta_df, max_pulse_count, batch_df, train=True):\n",
    "    # read metadata\n",
    "    batch_id, first_pulse_index, last_pulse_index = batch_meta_df.iloc[event_idx][[\"batch_id\", \"first_pulse_index\", \"last_pulse_index\"]].astype(\"int\")\n",
    "\n",
    "    # read event\n",
    "    event_feature = batch_df[first_pulse_index:last_pulse_index + 1]\n",
    "    sensor_id = event_feature.sensor_id\n",
    "    \n",
    "    # merge features into single structured array\n",
    "    dtype = [(\"time\", \"float16\"),\n",
    "             (\"charge\", \"float16\"),\n",
    "             (\"auxiliary\", \"float16\"),\n",
    "             (\"x\", \"float16\"),\n",
    "             (\"y\", \"float16\"),\n",
    "             (\"z\", \"float16\"),\n",
    "             (\"rank\", \"short\")]\n",
    "    event_x = np.zeros(last_pulse_index - first_pulse_index + 1, dtype)\n",
    "\n",
    "    event_x[\"time\"] = event_feature.time.values - event_feature.time.min()\n",
    "    event_x[\"charge\"] = event_feature.charge.values\n",
    "    event_x[\"auxiliary\"] = event_feature.auxiliary.values\n",
    "\n",
    "    event_x[\"x\"] = sensor_geometry_df.x[sensor_id].values\n",
    "    event_x[\"y\"] = sensor_geometry_df.y[sensor_id].values\n",
    "    event_x[\"z\"] = sensor_geometry_df.z[sensor_id].values\n",
    "\n",
    "    # For long event, pick-up\n",
    "    if len(event_x) > max_pulse_count:\n",
    "        # Find valid time window\n",
    "        t_peak = event_x[\"time\"][event_x[\"charge\"].argmax()]\n",
    "        t_valid_min = t_peak - t_valid_length\n",
    "        t_valid_max = t_peak + t_valid_length\n",
    "\n",
    "        t_valid = (event_x[\"time\"] > t_valid_min) * (event_x[\"time\"] < t_valid_max)\n",
    "\n",
    "        # rank\n",
    "        event_x[\"rank\"] = 2 * (1 - event_x[\"auxiliary\"]) + (t_valid)\n",
    "\n",
    "        # sort by rank and charge (important goes to backward)\n",
    "        event_x = np.sort(event_x, order=[\"rank\", \"charge\"])\n",
    "\n",
    "        # pick-up from backward\n",
    "        event_x = event_x[-max_pulse_count:]\n",
    "\n",
    "        # resort by time\n",
    "        event_x = np.sort(event_x, order=\"time\")\n",
    "\n",
    "    # resort by time\n",
    "    event_x = np.sort(event_x, order=\"time\")\n",
    "        \n",
    "    # for train data, give angles together\n",
    "    azimuth, zenith = batch_meta_df.iloc[event_idx][[\"azimuth\", \"zenith\"]].astype(\"float16\")\n",
    "    event_y = np.array([azimuth, zenith], dtype=\"float16\")\n",
    "        \n",
    "    return event_idx, len(event_x), event_x, event_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f574a55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T22:17:47.166158Z",
     "iopub.status.busy": "2023-03-15T22:17:47.165715Z",
     "iopub.status.idle": "2023-03-15T22:26:17.183034Z",
     "shell.execute_reply": "2023-03-15T22:26:17.181126Z"
    },
    "papermill": {
     "duration": 510.024388,
     "end_time": "2023-03-15T22:26:17.186493",
     "exception": false,
     "start_time": "2023-03-15T22:17:47.162105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading batch  100 DONE! Saving...\n",
      "Reading batch  101 DONE! Saving...\n",
      "Reading batch  102 DONE! Saving...\n",
      "Reading batch  103 DONE! Saving...\n",
      "Reading batch  104 DONE! Saving...\n",
      "Reading batch  105 DONE! Saving...\n",
      "Reading batch  106 DONE! Saving...\n",
      "Reading batch  107 DONE! Saving...\n",
      "Reading batch  108 DONE! Saving...\n",
      "Reading batch  109 DONE! Saving...\n",
      "Reading batch  110 DONE! Saving...\n",
      "Reading batch  111 DONE! Saving...\n",
      "Reading batch  112 DONE! Saving...\n",
      "Reading batch  113 DONE! Saving...\n",
      "Reading batch  114 DONE! Saving...\n",
      "Reading batch  115 DONE! Saving...\n",
      "Reading batch  116 DONE! Saving...\n",
      "Reading batch  117 DONE! Saving...\n",
      "Reading batch  118 DONE! Saving...\n",
      "Reading batch  119 DONE! Saving...\n",
      "Reading batch  120 DONE! Saving...\n",
      "Reading batch  121 DONE! Saving...\n",
      "Reading batch  122 DONE! Saving...\n",
      "Reading batch  123 DONE! Saving...\n",
      "Reading batch  124 DONE! Saving...\n",
      "Reading batch  125 DONE! Saving...\n",
      "Reading batch  126 DONE! Saving...\n",
      "Reading batch  127 DONE! Saving...\n",
      "Reading batch  128 DONE! Saving...\n",
      "Reading batch  129 DONE! Saving...\n",
      "Reading batch  130 DONE! Saving...\n",
      "Reading batch  131 DONE! Saving...\n",
      "Reading batch  132 DONE! Saving...\n",
      "Reading batch  133 DONE! Saving...\n",
      "Reading batch  134 DONE! Saving...\n",
      "Reading batch  135 DONE! Saving...\n",
      "Reading batch  136 DONE! Saving...\n",
      "Reading batch  137 DONE! Saving...\n",
      "Reading batch  138 DONE! Saving...\n",
      "Reading batch  139 DONE! Saving...\n",
      "Reading batch  140 DONE! Saving...\n",
      "Reading batch  141 DONE! Saving...\n",
      "Reading batch  142 DONE! Saving...\n",
      "Reading batch  143 DONE! Saving...\n",
      "Reading batch  144 DONE! Saving...\n",
      "Reading batch  145 DONE! Saving...\n",
      "Reading batch  146 DONE! Saving...\n",
      "Reading batch  147 DONE! Saving...\n",
      "Reading batch  148 DONE! Saving...\n",
      "Reading batch  149 DONE! Saving...\n",
      "Reading batch  150 DONE! Saving...\n",
      "Reading batch  151 DONE! Saving...\n",
      "Reading batch  152 DONE! Saving...\n",
      "Reading batch  153 DONE! Saving...\n",
      "Reading batch  154 DONE! Saving...\n",
      "Reading batch  155 DONE! Saving...\n",
      "Reading batch  156 DONE! Saving...\n",
      "Reading batch  157 DONE! Saving...\n",
      "Reading batch  158 DONE! Saving...\n",
      "Reading batch  159 DONE! Saving...\n",
      "Reading batch  160 DONE! Saving...\n",
      "Reading batch  161 DONE! Saving...\n",
      "Reading batch  162 DONE! Saving...\n",
      "Reading batch  163 DONE! Saving...\n",
      "Reading batch  164 DONE! Saving...\n",
      "Reading batch  165 DONE! Saving...\n",
      "Reading batch  166 DONE! Saving...\n",
      "Reading batch  167 DONE! Saving...\n",
      "Reading batch  168 DONE! Saving...\n",
      "Reading batch  169 DONE! Saving...\n",
      "Reading batch  170 DONE! Saving...\n",
      "Reading batch  171 DONE! Saving...\n",
      "Reading batch  172 DONE! Saving...\n",
      "Reading batch  173 DONE! Saving...\n",
      "Reading batch  174 DONE! Saving...\n",
      "Reading batch  175 DONE! Saving...\n",
      "Reading batch  176 DONE! Saving...\n",
      "Reading batch  177 DONE! Saving...\n",
      "Reading batch  178 DONE! Saving...\n",
      "Reading batch  179 DONE! Saving...\n",
      "Reading batch  180 DONE! Saving...\n",
      "Reading batch  181 DONE! Saving...\n",
      "Reading batch  182 DONE! Saving...\n",
      "Reading batch  183 DONE! Saving...\n",
      "Reading batch  184 DONE! Saving...\n",
      "Reading batch  185 DONE! Saving...\n",
      "Reading batch  186 DONE! Saving...\n",
      "Reading batch  187 DONE! Saving...\n",
      "Reading batch  188 DONE! Saving...\n",
      "Reading batch  189 DONE! Saving...\n",
      "Reading batch  190 DONE! Saving...\n",
      "Reading batch  191 DONE! Saving...\n",
      "Reading batch  192 DONE! Saving...\n",
      "Reading batch  193 DONE! Saving...\n",
      "Reading batch  194 DONE! Saving...\n",
      "Reading batch  195 DONE! Saving...\n",
      "Reading batch  196 DONE! Saving...\n",
      "Reading batch  197 DONE! Saving...\n",
      "Reading batch  198 DONE! Saving...\n",
      "Reading batch  199 DONE! Saving...\n",
      "Reading batch  200 DONE! Saving...\n"
     ]
    }
   ],
   "source": [
    "# Read Train Meta Data\n",
    "train_meta_df = pd.read_parquet(home_dir + 'train_meta.parquet')\n",
    "\n",
    "batch_counts = train_meta_df.batch_id.value_counts().sort_index()\n",
    "\n",
    "batch_max_index = batch_counts.cumsum()\n",
    "batch_max_index[train_meta_df.batch_id.min() - 1] = 0\n",
    "batch_max_index = batch_max_index.sort_index()\n",
    "\n",
    "def train_meta_df_spliter(batch_id):\n",
    "    return train_meta_df.loc[batch_max_index[batch_id - 1]:batch_max_index[batch_id] - 1]\n",
    "\n",
    "for batch_id in train_batch_ids:\n",
    "    print(\"Reading batch \", batch_id, end=\"\")\n",
    "    # get batch meta data and data\n",
    "    batch_meta_df = train_meta_df_spliter(batch_id)\n",
    "    batch_df = pd.read_parquet(train_format.format(batch_id=batch_id))\n",
    "\n",
    "    # register pulses\n",
    "    batch_x = np.zeros((len(batch_meta_df), max_pulse_count, n_features), dtype=\"float16\")\n",
    "    batch_y = np.zeros((len(batch_meta_df), 2), dtype=\"float16\")\n",
    "    \n",
    "    batch_x[:, :, 2] = -1\n",
    "\n",
    "    def read_event_local(event_idx):\n",
    "        return read_event(event_idx, batch_meta_df, max_pulse_count, batch_df, train=True)\n",
    "\n",
    "    # Proces Events\n",
    "    iterator = range(len(batch_meta_df))\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        for event_idx, pulse_count, event_x, event_y in pool.map(read_event_local, iterator):\n",
    "            batch_x[event_idx, :pulse_count, 0] = event_x[\"time\"]\n",
    "            batch_x[event_idx, :pulse_count, 1] = event_x[\"charge\"]\n",
    "            batch_x[event_idx, :pulse_count, 2] = event_x[\"auxiliary\"]\n",
    "            batch_x[event_idx, :pulse_count, 3] = event_x[\"x\"]\n",
    "            batch_x[event_idx, :pulse_count, 4] = event_x[\"y\"]\n",
    "            batch_x[event_idx, :pulse_count, 5] = event_x[\"z\"]\n",
    "            batch_x[event_idx, :pulse_count, 6] = event_x[\"rank\"]\n",
    "\n",
    "            batch_y[event_idx] = event_y\n",
    "\n",
    "    del batch_meta_df, batch_df\n",
    "    \n",
    "    # Save\n",
    "    print(\" DONE! Saving...\")\n",
    "    np.savez(point_picker_format.format(batch_id=batch_id), x=batch_x, y=batch_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 520.529497,
   "end_time": "2023-03-15T22:26:18.419916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-15T22:17:37.890419",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
